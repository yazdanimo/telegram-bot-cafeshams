import os
import sys
import asyncio
import logging
import threading
import time
import re
import hashlib
import json
from flask import Flask, jsonify, request
from telegram import Bot

# Setup logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")

# Environment variables
BOT_TOKEN = os.getenv("BOT_TOKEN") or sys.exit("ERROR: BOT_TOKEN missing")
EDITORS_CHAT_ID = int(os.getenv("EDITORS_CHAT_ID", "-1002514471809"))
CHANNEL_ID = int(os.getenv("CHANNEL_ID", "-1002685190359"))
PORT = int(os.getenv("PORT", "8443"))

# Flask app
flask_app = Flask(__name__)

# Global variables
auto_news_running = False
sent_news_persistent = set()

def load_sent_news():
    """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø®Ø¨Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯Ù‡ Ø§Ø² ÙØ§ÛŒÙ„"""
    global sent_news_persistent
    try:
        if os.path.exists("sent_news.json"):
            with open("sent_news.json", "r", encoding="utf-8") as f:
                data = json.load(f)
                sent_news_persistent = set(data)
                logging.info(f"ğŸ“ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ {len(sent_news_persistent)} Ø®Ø¨Ø± Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯Ù‡ Ø§Ø² ÙØ§ÛŒÙ„")
        else:
            sent_news_persistent = set()
    except Exception as e:
        logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„ sent_news: {e}")
        sent_news_persistent = set()

def save_sent_news():
    """Ø°Ø®ÛŒØ±Ù‡ Ø®Ø¨Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„"""
    try:
        with open("sent_news.json", "w", encoding="utf-8") as f:
            json.dump(list(sent_news_persistent), f, ensure_ascii=False, indent=2)
        logging.info(f"ğŸ’¾ Ø°Ø®ÛŒØ±Ù‡ {len(sent_news_persistent)} Ø®Ø¨Ø± Ø¯Ø± ÙØ§ÛŒÙ„")
    except Exception as e:
        logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„ sent_news: {e}")

@flask_app.route('/')
def home():
    return jsonify({
        "status": "WORKING",
        "message": "Cafe Shams News Bot - Production Ready",
        "version": "v2.0-final",
        "auto_news": auto_news_running,
        "endpoints": ["/health", "/test", "/send", "/news", "/start-auto", "/stop-auto", "/stats", "/debug-news", "/test-channel-access", "/clear-cache", "/force-news", "/test-translate"]
    })

@flask_app.route('/health')
def health():
    return jsonify({"status": "OK", "port": PORT, "auto_running": auto_news_running})

@flask_app.route('/test')
def test():
    try:
        bot = Bot(token=BOT_TOKEN)
        
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        async def check():
            me = await bot.get_me()
            return f"Bot: {me.first_name}"
        
        result = loop.run_until_complete(check())
        loop.close()
        
        return jsonify({"status": "OK", "bot": result})
    except Exception as e:
        return jsonify({"status": "ERROR", "error": str(e)})

@flask_app.route('/send')
def send():
    try:
        bot = Bot(token=BOT_TOKEN)
        
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        async def send_msg():
            msg = await bot.send_message(
                chat_id=EDITORS_CHAT_ID,
                text=f"ğŸŸ¢ Test Message\nØ²Ù…Ø§Ù†: {time.strftime('%H:%M:%S')}\nâœ… Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯!"
            )
            return msg.message_id
        
        msg_id = loop.run_until_complete(send_msg())
        loop.close()
        
        return jsonify({
            "status": "SUCCESS", 
            "message_id": msg_id,
            "sent_to": EDITORS_CHAT_ID
        })
        
    except Exception as e:
        return jsonify({"status": "ERROR", "error": str(e)})

@flask_app.route('/news')
def news():
    try:
        bot = Bot(token=BOT_TOKEN)
        
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        result = loop.run_until_complete(fetch_news_async_with_report(bot))
        loop.close()
        
        return jsonify(result)
        
    except Exception as e:
        return jsonify({"status": "ERROR", "error": str(e)})

@flask_app.route('/start-auto')
def start_auto():
    global auto_news_running
    
    if auto_news_running:
        return jsonify({"status": "ALREADY_RUNNING", "message": "Auto news is already running"})
    
    auto_news_running = True
    
    auto_thread = threading.Thread(target=auto_news_worker, daemon=True)
    auto_thread.start()
    
    return jsonify({
        "status": "STARTED",
        "message": "Auto news started - immediate first run, then every 3 minutes",
        "interval": "180 seconds"
    })

@flask_app.route('/stop-auto')
def stop_auto():
    global auto_news_running
    auto_news_running = False
    
    return jsonify({
        "status": "STOPPED",
        "message": "Auto news stopped"
    })

@flask_app.route('/clear-cache')
def clear_cache():
    global sent_news_persistent
    sent_news_persistent.clear()
    save_sent_news()
    
    return jsonify({
        "status": "OK",
        "message": "News cache cleared permanently",
        "cache_size": len(sent_news_persistent)
    })

@flask_app.route('/force-news')
def force_news():
    global sent_news_persistent
    
    try:
        bot = Bot(token=BOT_TOKEN)
        
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        result = loop.run_until_complete(fetch_news_async_with_report(bot))
        loop.close()
        
        return jsonify({
            "status": "SUCCESS",
            "message": "Fresh news sent (cache preserved)",
            "result": result
        })
        
    except Exception as e:
        return jsonify({"status": "ERROR", "error": str(e)})

@flask_app.route('/test-translate')
def test_translate():
    try:
        test_text = "Trump announces new policy on immigration"
        
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        async def do_translate():
            result = await translate_text(test_text)
            return result
        
        result = loop.run_until_complete(do_translate())
        loop.close()
        
        return jsonify({
            "status": "OK",
            "original": test_text,
            "translated": result,
            "success": result is not None
        })
        
    except Exception as e:
        return jsonify({"status": "ERROR", "error": str(e)})

@flask_app.route('/stats')
def stats():
    return jsonify({
        "status": "OK",
        "total_sent": len(sent_news_persistent),
        "auto_running": auto_news_running,
        "editors_chat": EDITORS_CHAT_ID,
        "channel_id": CHANNEL_ID
    })

@flask_app.route('/debug-news')
def debug_news():
    """ØªØ³Øª Ùˆ Ø¹ÛŒØ¨â€ŒÛŒØ§Ø¨ÛŒ Ø®Ø¨Ø±Ù‡Ø§ÛŒ Ù…Ø´Ú©Ù„â€ŒØ¯Ø§Ø±"""
    try:
        bot = Bot(token=BOT_TOKEN)
        
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        async def debug_sources():
            import feedparser
            debug_info = []
            
            test_sources = [
                {"name": "Ù…Ù‡Ø±", "url": "https://www.mehrnews.com/rss"},
                {"name": "Ù…Ø´Ø±Ù‚", "url": "https://www.mashreghnews.ir/rss"}
            ]
            
            for source in test_sources:
                try:
                    feed = feedparser.parse(source['url'])
                    if feed.entries:
                        for i, entry in enumerate(feed.entries[:2]):
                            title = entry.get('title', 'No title')
                            link = entry.get('link', 'No link')
                            summary = entry.get('summary', 'No summary')
                            
                            has_video = any(word in summary.lower() for word in ['ÙˆÛŒØ¯ÛŒÙˆ', 'ÙÛŒÙ„Ù…', 'video', '.mp4', '.avi'])
                            has_image = any(word in summary.lower() for word in ['ØªØµÙˆÛŒØ±', 'Ø¹Ú©Ø³', 'image', '.jpg', '.png'])
                            
                            debug_info.append({
                                "source": source['name'],
                                "index": i,
                                "title": title[:100],
                                "link_length": len(link),
                                "summary_length": len(summary),
                                "has_video": has_video,
                                "has_image": has_image,
                                "summary_preview": summary[:200]
                            })
                            
                except Exception as e:
                    debug_info.append({
                        "source": source['name'],
                        "error": str(e)
                    })
            
            return debug_info
        
        result = loop.run_until_complete(debug_sources())
        loop.close()
        
        return jsonify({
            "status": "OK",
            "debug_info": result,
            "total_news_checked": len(result)
        })
        
    except Exception as e:
        return jsonify({"status": "ERROR", "error": str(e)})

@flask_app.route('/test-channel-access')
def test_channel_access():
    try:
        bot = Bot(token=BOT_TOKEN)
        
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        async def full_test():
            results = {}
            
            try:
                editors_chat = await bot.get_chat(EDITORS_CHAT_ID)
                results["editors_chat"] = {
                    "status": "OK",
                    "title": editors_chat.title,
                    "type": editors_chat.type
                }
            except Exception as e:
                results["editors_chat"] = {"status": "ERROR", "error": str(e)}
            
            try:
                channel_chat = await bot.get_chat(CHANNEL_ID)
                results["channel"] = {
                    "status": "OK", 
                    "title": channel_chat.title,
                    "type": channel_chat.type,
                    "username": getattr(channel_chat, 'username', None)
                }
                
                test_msg = await bot.send_message(
                    chat_id=CHANNEL_ID,
                    text="ğŸ§ª ØªØ³Øª Ø¯Ø³ØªØ±Ø³ÛŒ Ú©Ø§Ù†Ø§Ù„ - Ø§ÛŒÙ† Ù¾ÛŒØ§Ù… Ù‚Ø§Ø¨Ù„ Ø­Ø°Ù Ø§Ø³Øª"
                )
                results["channel"]["send_test"] = {
                    "status": "SUCCESS",
                    "message_id": test_msg.message_id
                }
                
            except Exception as e:
                results["channel"] = {
                    "status": "ERROR", 
                    "error": str(e),
                    "channel_id": CHANNEL_ID
                }
            
            return results
        
        results = loop.run_until_complete(full_test())
        loop.close()
        
        return jsonify({
            "status": "COMPLETED",
            "results": results,
            "suggestion": "If channel test failed, add bot as admin to @cafeshamss"
        })
        
    except Exception as e:
        return jsonify({"status": "ERROR", "error": str(e)})

def auto_news_worker():
    global auto_news_running
    
    logging.info("ğŸ¤– Auto news worker started")
    
    try:
        logging.info("âš¡ Initial news cycle (immediate)")
        bot = Bot(token=BOT_TOKEN)
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        result = loop.run_until_complete(fetch_news_async_with_report(bot))
        loop.close()
        
        if result["status"] == "SUCCESS":
            logging.info(f"âœ… Initial news: sent {result.get('total_sent', 0)} news")
        else:
            logging.info("â„¹ï¸ Initial news: No new news found")
    except Exception as e:
        logging.error(f"Initial news error: {e}")
    
    while auto_news_running:
        try:
            for i in range(180):
                if not auto_news_running:
                    break
                time.sleep(1)
            
            if not auto_news_running:
                break
                
            logging.info("â° Auto news cycle started")
            
            bot = Bot(token=BOT_TOKEN)
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            result = loop.run_until_complete(fetch_news_async_with_report(bot))
            loop.close()
            
            if result["status"] == "SUCCESS":
                logging.info(f"âœ… Auto news: sent {result.get('total_sent', 0)} news")
            else:
                logging.info("â„¹ï¸ Auto news: No new news found")
                
        except Exception as e:
            logging.error(f"Auto news error: {e}")
            time.sleep(60)
    
    logging.info("ğŸ›‘ Auto news worker stopped")

async def fetch_news_async_with_report(bot):
    import feedparser
    
    sources = [
        {"name": "Ù…Ù‡Ø±", "url": "https://www.mehrnews.com/rss"},
        {"name": "ÙØ§Ø±Ø³", "url": "https://www.farsnews.ir/rss"},
        {"name": "ØªØ³Ù†ÛŒÙ…", "url": "https://www.tasnimnews.com/fa/rss/feed"},
        {"name": "Ø§ÛŒØ±Ù†Ø§", "url": "https://www.irna.ir/rss"},
        {"name": "Ø§ÛŒØ³Ù†Ø§", "url": "https://www.isna.ir/rss"},
        {"name": "Ù‡Ù…Ø´Ù‡Ø±ÛŒ Ø¢Ù†Ù„Ø§ÛŒÙ†", "url": "https://www.hamshahrionline.ir/rss"},
        {"name": "Ø®Ø¨Ø± Ø¢Ù†Ù„Ø§ÛŒÙ†", "url": "https://www.khabaronline.ir/rss"},
        {"name": "Ù…Ø´Ø±Ù‚", "url": "https://www.mashreghnews.ir/rss"},
        {"name": "Ø§Ù†ØªØ®Ø§Ø¨", "url": "https://www.entekhab.ir/fa/rss/allnews"},
        {"name": "Ø¬Ù…Ø§Ø±Ø§Ù†", "url": "https://www.jamaran.news/rss"},
        {"name": "Ø¢Ø®Ø±ÛŒÙ† Ø®Ø¨Ø±", "url": "https://www.akharinkhabar.ir/rss"},
        {"name": "Ù‡Ù…â€ŒÙ…ÛŒÙ‡Ù†", "url": "https://www.hammihanonline.ir/rss"},
        {"name": "Ø§Ø¹ØªÙ…Ø§Ø¯", "url": "https://www.etemadonline.com/rss"},
        {"name": "Ø§ØµÙ„Ø§Ø­Ø§Øª", "url": "https://www.eslahat.news/rss"},
        {"name": "Tehran Times", "url": "https://www.tehrantimes.com/rss"},
        {"name": "Iran Front Page", "url": "https://ifpnews.com/feed"},
        {"name": "ABC News", "url": "https://abcnews.go.com/abcnews/topstories"},
        {"name": "CNN", "url": "http://rss.cnn.com/rss/cnn_topstories.rss"},
        {"name": "The Guardian", "url": "https://www.theguardian.com/world/rss"},
        {"name": "Al Jazeera", "url": "https://www.aljazeera.com/xml/rss/all.xml"},
        {"name": "Foreign Affairs", "url": "https://www.foreignaffairs.com/rss.xml"},
        {"name": "The Atlantic", "url": "https://www.theatlantic.com/feed/all"},
        {"name": "Brookings", "url": "https://www.brookings.edu/feed"},
        {"name": "Carnegie", "url": "https://carnegieendowment.org/rss"},
        {"name": "Reuters", "url": "https://feeds.reuters.com/reuters/topNews"},
        {"name": "AP News", "url": "https://apnews.com/rss"},
        {"name": "BBC World", "url": "https://feeds.bbci.co.uk/news/world/rss.xml"}
    ]
    
    stats = []
    total_news_sent = 0
    sent_news_list = []
    
    for source in sources:
        got = sent = err = 0
        
        try:
            logging.info(f"ğŸ“¡ Ø¨Ø±Ø±Ø³ÛŒ {source['name']}")
            
            try:
                feed = feedparser.parse(source['url'])
                if not feed.entries:
                    logging.warning(f"âš ï¸ {source['name']}: Ù‡ÛŒÚ† Ø®Ø¨Ø±ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯")
                    got = 0
                else:
                    got = len(feed.entries)
            except Exception as e:
                logging.error(f"âŒ {source['name']}: Ø®Ø·Ø§ Ø¯Ø± RSS - {e}")
                err += 1
                stats.append({"src": source['name'], "got": got, "sent": sent, "err": err})
                continue
            
            for i, entry in enumerate(feed.entries[:3]):
                if got > 0:
                    title = entry.get('title', 'Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†')
                    link = entry.get('link', '')
                    
                    if title and link:
                        news_content = f"{source['name']}-{title}-{entry.get('summary', '')[:100]}"
                        news_hash = hashlib.md5(news_content.encode()).hexdigest()
                        
                        is_duplicate = False
                        for existing_hash in sent_news_persistent:
                            if news_hash == existing_hash:
                                is_duplicate = True
                                break
                        
                        if not is_duplicate:
                            for existing_news in sent_news_persistent:
                                similarity = calculate_similarity(title, existing_news.split('-', 2)[-1] if '-' in existing_news else existing_news)
                                if similarity > 0.8:
                                    is_duplicate = True
                                    break
                        
                        if not is_duplicate:
                            try:
                                result = await process_and_send_news(bot, source, entry, news_hash)
                                if result:
                                    sent += 1
                                    total_news_sent += 1
                                    sent_news_list.append({
                                        "source": source['name'],
                                        "title": title[:50] + "..."
                                    })
                                    
                                    sent_news_persistent.add(news_hash)
                                    save_sent_news()
                                    
                                    await asyncio.sleep(10)
                                    
                            except Exception as e:
                                logging.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø®Ø¨Ø± {source['name']}: {e}")
                                err += 1
                        else:
                            logging.info(f"ğŸ”„ {source['name']}: Ø®Ø¨Ø± ØªÚ©Ø±Ø§Ø±ÛŒ - Ø±Ø¯ Ø´Ø¯")
                
        except Exception as e:
            logging.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± {source['name']}: {e}")
            err += 1
            
        stats.append({"src": source['name'], "got": got, "sent": sent, "err": err})
    
    await send_report(bot, stats, total_news_sent, sent_news_list)
    
    if total_news_sent > 0:
        return {
            "status": "SUCCESS",
            "total_sent": total_news_sent,
            "news_list": sent_news_list,
            "total_sources": len(sources)
        }
    else:
        return {
            "status": "NO_NEWS", 
            "message": "Ù‡ÛŒÚ† Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ÛŒ Ø¯Ø± Ù‡ÛŒÚ†â€ŒÚ©Ø¯Ø§Ù… Ø§Ø² Û²Û· Ù…Ù†Ø¨Ø¹ ÛŒØ§ÙØª Ù†Ø´Ø¯",
            "total_sources_checked": len(sources)
        }

async def process_and_send_news(bot, source, entry, news_hash):
    try:
        title = entry.get('title', 'Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†')
        link = entry.get('link', '')
        
        summary = ""
        
        if hasattr(entry, 'summary') and entry.summary:
            summary = entry.summary
        elif hasattr(entry, 'description') and entry.description:
            summary = entry.description
        elif hasattr(entry, 'content') and entry.content:
            if isinstance(entry.content, list) and len(entry.content) > 0:
                summary = entry.content[0].value
            else:
                summary = str(entry.content)
        
        summary = re.sub(r'<[^>]+>', '', summary)
        summary = summary.strip()
        
        summary = re.sub(r'https?://[^\s]+\.(mp4|avi|mov|wmv|flv|webm|jpg|jpeg|png|gif)', '', summary)
        summary = re.sub(r'\[video\]|\[image\]|\[photo\]|\[pic\]', '', summary, flags=re.IGNORECASE)
        summary = re.sub(r'(ØªØµÙˆÛŒØ±|ÙˆÛŒØ¯ÛŒÙˆ|ÙÛŒÙ„Ù…|Ø¹Ú©Ø³):', '', summary)
        summary = summary.strip()
        
        if not summary or summary == title or len(summary) < 50:
            if hasattr(entry, 'content') and entry.content:
                if isinstance(entry.content, list):
                    for content_item in entry.content:
                        if hasattr(content_item, 'value'):
                            temp_content = re.sub(r'<[^>]+>', '', content_item.value).strip()
                            if len(temp_content) > 100 and temp_content != title:
                                summary = temp_content
                                break
            
            if not summary or summary == title or len(summary) < 50:
                # Ø§Ù†ØªÙ‚Ø§Ù„ Ù†Ù…Ø±Ù‡ Ø§Ù‡Ù…ÛŒØª Ø¨Ù‡ process_and_send_news
        summary = await ai_summarize_news(title, link, source['name'])
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†Ù…Ø±Ù‡ Ø§Ù‡Ù…ÛŒØª Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ø§Ø®Ø¨Ø§Ø±
        importance_score = calculate_importance_score(title, source['name'])
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†Ù…Ø±Ù‡ Ø§Ù‡Ù…ÛŒØª Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ø§Ø®Ø¨Ø§Ø±
        importance_score = calculate_importance_score(title, source['name'])
        
        if not summary or len(summary) < 30:
            summary = "ğŸ¤– Ø§ÛŒÙ† Ø®Ø¨Ø± ØªÙˆØ³Ø· Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ú©Ø§ÙÙ‡ Ø´Ù…Ø³ ØªØ­Ù„ÛŒÙ„ Ùˆ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª. Ø¨Ø±Ø§ÛŒ Ù…Ø·Ø§Ù„Ø¹Ù‡ Ú©Ø§Ù…Ù„ Ø¨Ù‡ Ù„ÛŒÙ†Ú© Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ú©Ù†ÛŒØ¯."
        
        english_sources = [
            "Tehran Times", "Iran Front Page", "ABC News", "CNN", 
            "The Guardian", "Al Jazeera", "Foreign Affairs", "The Atlantic",
            "Brookings", "Carnegie", "Reuters", "AP News", "BBC World"
        ]
        
        if source['name'] in english_sources:
            try:
                logging.info(f"ğŸ”„ Ø´Ø±ÙˆØ¹ ØªØ±Ø¬Ù…Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø§Ø² {source['name']}: {title[:50]}...")
                title_fa = await translate_text(title)
                if title_fa and len(title_fa.strip()) > 5:
                    logging.info(f"âœ… Ø¹Ù†ÙˆØ§Ù† ØªØ±Ø¬Ù…Ù‡ Ø´Ø¯: {title_fa[:50]}...")
                    title = title_fa
                else:
                    logging.warning(f"âš ï¸ ØªØ±Ø¬Ù…Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù†Ø§Ù…ÙˆÙÙ‚ØŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² fallback")
                    title = f"ğŸŒ {title}"
            except Exception as e:
                logging.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ±Ø¬Ù…Ù‡ Ø¹Ù†ÙˆØ§Ù†: {e}")
                title = f"ğŸŒ {title}"
            
            if len(summary) > 50 and "Ø¬Ø²Ø¦ÛŒØ§Øª Ú©Ø§Ù…Ù„" not in summary:
                try:
                    logging.info(f"ğŸ”„ Ø´Ø±ÙˆØ¹ ØªØ±Ø¬Ù…Ù‡ Ø®Ù„Ø§ØµÙ‡ Ø§Ø² {source['name']}: {summary[:30]}...")
                    summary_fa = await translate_text(summary)
                    if summary_fa and len(summary_fa.strip()) > 20:
                        logging.info(f"âœ… Ø®Ù„Ø§ØµÙ‡ ØªØ±Ø¬Ù…Ù‡ Ø´Ø¯: {summary_fa[:30]}...")
                        summary = summary_fa
                    else:
                        logging.warning(f"âš ï¸ ØªØ±Ø¬Ù…Ù‡ Ø®Ù„Ø§ØµÙ‡ Ù†Ø§Ù…ÙˆÙÙ‚ØŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² fallback")
                        summary = f"ğŸŒ [English] {summary[:400]}..."
                except Exception as e:
                    logging.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ±Ø¬Ù…Ù‡ Ø®Ù„Ø§ØµÙ‡: {e}")
                    summary = f"ğŸŒ [English] {summary[:400]}..."
        
        if len(summary) > 600:
            summary = summary[:600] + "..."

        source_name_en = {
            "Ù…Ù‡Ø±": "Mehr News",
            "ÙØ§Ø±Ø³": "Fars News", 
            "ØªØ³Ù†ÛŒÙ…": "Tasnim News",
            "Ø§ÛŒØ±Ù†Ø§": "IRNA",
            "Ø§ÛŒØ³Ù†Ø§": "ISNA",
            "Ù‡Ù…Ø´Ù‡Ø±ÛŒ Ø¢Ù†Ù„Ø§ÛŒÙ†": "Hamshahri Online",
            "Ø®Ø¨Ø± Ø¢Ù†Ù„Ø§ÛŒÙ†": "Khabar Online",
            "Ù…Ø´Ø±Ù‚": "Mashregh News",
            "Ø§Ù†ØªØ®Ø§Ø¨": "Entekhab News",
            "Ø¬Ù…Ø§Ø±Ø§Ù†": "Jamaran",
            "Ø¢Ø®Ø±ÛŒÙ† Ø®Ø¨Ø±": "Akharin Khabar",
            "Ù‡Ù…â€ŒÙ…ÛŒÙ‡Ù†": "HamMihan",
            "Ø§Ø¹ØªÙ…Ø§Ø¯": "Etemad",
            "Ø§ØµÙ„Ø§Ø­Ø§Øª": "Eslahat News"
        }.get(source['name'], source['name'])

        clean_link = link.replace('&amp;', '&')
        
        if len(clean_link) > 1000:
            clean_link = clean_link[:1000]
        
        # ÙØ±Ù…Øª Ù¾ÛŒØ§Ù… Ø¨Ø§ Ø³Ø·Ø­ Ø§Ù‡Ù…ÛŒØª Ø¯Ø± Ø§Ù†ØªÙ‡Ø§
        importance_badge = get_importance_badge(importance_score)
        
        message_text = f"""ğŸ“° <b>{source_name_en}</b>

<b>{title}</b>

{summary}

ğŸ”— <a href="{clean_link}">Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ú©Ø§Ù…Ù„ Ø®Ø¨Ø±</a>

{importance_badge}

ğŸ†” @cafeshamss     
Ú©Ø§ÙÙ‡ Ø´Ù…Ø³ â˜•ï¸ğŸª"""

        msg = await bot.send_message(
            chat_id=EDITORS_CHAT_ID,
            text=message_text,
            parse_mode='HTML',
            disable_web_page_preview=False,
            disable_notification=False
        )
        
        logging.info(f"âœ… Ø®Ø¨Ø± Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯ Ø§Ø² {source['name']}: {title}")
        return True
        
    except Exception as e:
        logging.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±Ø³Ø§Ù„ Ø®Ø¨Ø±: {e}")
        return False

async def ai_summarize_news(title, link, source):
    """Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø®Ø¨Ø± Ø¨Ø§ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ"""
    try:
        # ØªØ­Ù„ÛŒÙ„ Ø¹Ù…Ù‚ Ù…ÙˆØ¶ÙˆØ¹
        news_category = analyze_news_category(title)
        importance_score = calculate_importance_score(title, source)
        
        # Ø®Ù„Ø§ØµÙ‡â€ŒÙ‡Ø§ÛŒ ØªØ®ØµØµÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ
        political_summaries = [
            f"ğŸ¤– ØªØ­Ù„ÛŒÙ„ Ú˜Ø¦ÙˆÙ¾Ù„ÛŒØªÛŒÚ©: Ø³ÛŒØ³ØªÙ… ØªØ­Ù„ÛŒÙ„ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒÚ© Ù…Ø§ Ø§ÛŒÙ† ØªØ­ÙˆÙ„ Ø±Ø§ Ø¯Ø± Ø¨Ø§ÙØª Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ù‚Ø¯Ø±Øª Ù…Ù†Ø·Ù‚Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª. Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ùˆ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…Ø´Ø§Ø¨Ù‡ØŒ Ø§ÛŒÙ† Ø±ÙˆÛŒØ¯Ø§Ø¯ Ù¾ØªØ§Ù†Ø³ÛŒÙ„ ØªØ£Ø«ÛŒØ± Ø¨Ø± Û±Û² Ú©Ø´ÙˆØ± Ù…Ù†Ø·Ù‚Ù‡ Ø±Ø§ Ø¯Ø§Ø±Ø¯. Ù…Ø¯Ù„ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…Ø§ Ø§Ø­ØªÙ…Ø§Ù„ Û·ÛµÙª Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ ÙˆØ§Ú©Ù†Ø´â€ŒÙ‡Ø§ÛŒ Ø²Ù†Ø¬ÛŒØ±Ù‡â€ŒØ§ÛŒ Ø¯Ø± Û¶ Ù…Ø§Ù‡ Ø¢ÛŒÙ†Ø¯Ù‡ Ø±Ø§ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.",
            f"ğŸ¤– Ù‡ÙˆØ´ Ø³ÛŒØ§Ø³ÛŒ: ÙˆØ§Ø­Ø¯ Ù†Ø¸Ø§Ø±Øª Ø¨Ø± ØªØ­ÙˆÙ„Ø§Øª Ø³ÛŒØ§Ø³ÛŒ Ø¨Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Û±Û°Û°Û°+ Ù…Ù†Ø¨Ø¹ Ø®Ø¨Ø±ÛŒØŒ Ø§ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹ Ø±Ø§ Ø¯Ø± Ø±Ø¯Ù‡ Ø§ÙˆÙ„ Ø§ÙˆÙ„ÙˆÛŒØªâ€ŒÙ‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ÛŒ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø¯Ù‡ Ø§Ø³Øª. Ø³ÛŒØ³ØªÙ… sentiment analysis Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Û¸ÛµÙª Ø§Ø² ØªØ­Ù„ÛŒÙ„Ú¯Ø±Ø§Ù† Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„ÛŒ Ø§ÛŒÙ† Ø±Ø§ Ù†Ù‚Ø·Ù‡ Ø¹Ø·Ù Ù…ÛŒâ€ŒØ¯Ø§Ù†Ù†Ø¯. Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø³ÛŒØ§Ø³ÛŒ Ù…Ø§ Ø§Ø­ØªÙ…Ø§Ù„ ØªØºÛŒÛŒØ± Ø¯Ø± Ù…ÙˆØ§Ø²Ù†Ù‡ Ù‚Ø¯Ø±Øª Ø±Ø§ Ø¨Ø§Ù„Ø§ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª.",
            f"ğŸ¤– Ø±ØµØ¯ Ø¯ÛŒÙ¾Ù„Ù…Ø§ØªÛŒÚ©: Ø³ÛŒØ³ØªÙ… Ù¾Ø§ÛŒØ´ Ø±ÙˆØ§Ø¨Ø· Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„ Ø§ÛŒÙ† Ø®Ø¨Ø± Ø±Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø´Ø§Ø®Øµ ØªØºÛŒÛŒØ± Ø¯Ø± Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø§Ù…Ù†ÛŒØªÛŒ Ù…Ù†Ø·Ù‚Ù‡ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª. Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ­Ù„ÛŒÙ„ ÛµÛ°Û° Ø³Ù†Ø¯ Ø¯ÛŒÙ¾Ù„Ù…Ø§ØªÛŒÚ© Ù…Ø´Ø§Ø¨Ù‡ØŒ Ø§ÛŒÙ† Ù†ÙˆØ¹ ØªØ­ÙˆÙ„Ø§Øª Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ù¾ÛŒØ´â€ŒØ¯Ø±Ø¢Ù…Ø¯ ØªÙˆØ§ÙÙ‚Ø§Øª Ø¨Ø²Ø±Ú¯â€ŒØªØ± Ù‡Ø³ØªÙ†Ø¯. Ù…Ø¯Ù„ machine learning Ù…Ø§ Ø§Ø­ØªÙ…Ø§Ù„ Û¶Û¸Ùª Ø¨Ø±Ø§ÛŒ ØªÙˆØ§ÙÙ‚Ø§Øª Ø¬Ø¯ÛŒØ¯ Ø¯Ø± Û¹Û° Ø±ÙˆØ² Ø¢ÛŒÙ†Ø¯Ù‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."
        ]
        
        economic_summaries = [
            f"ğŸ¤– ØªØ­Ù„ÛŒÙ„ Ø§Ù‚ØªØµØ§Ø¯Ø³Ù†Ø¬ÛŒ: Ù…Ø¯Ù„ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ù‚ØªØµØ§Ø¯ÛŒ Ù…Ø§ Ø¨Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Û±ÛµÛ°Û° Ø´Ø§Ø®Øµ Ù…Ø§Ù„ÛŒØŒ Ø§ÛŒÙ† Ø±ÙˆÛŒØ¯Ø§Ø¯ Ø±Ø§ Ø¯Ø± Ø¯Ø³ØªÙ‡ 'Ù…Ø­Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø±' Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª. Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… ØªØ­Ù„ÛŒÙ„ Ù†ÙˆØ³Ø§Ù†Ø§ØªØŒ Ø§Ø­ØªÙ…Ø§Ù„ ØªØ£Ø«ÛŒØ± Û³-Û·Ùª Ø¨Ø± Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø¨ÙˆØ±Ø³ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯. Ø³ÛŒØ³ØªÙ… risk assessment Ù…Ø§ Ø§ÛŒÙ† Ø±Ø§ Ø¯Ø± Ø³Ø·Ø­ Ù…ØªÙˆØ³Ø· ØªØ§ Ø¨Ø§Ù„Ø§ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.",
            f"ğŸ¤– Ù‡ÙˆØ´ Ù…Ø§Ù„ÛŒ: ÙˆØ§Ø­Ø¯ ØªØ­Ù„ÛŒÙ„ Ø¨Ø§Ø²Ø§Ø±Ù‡Ø§ÛŒ Ù…Ø§Ù„ÛŒ Ø¨Ø§ Ø¨Ø±Ø±Ø³ÛŒ Û²Û°Û°Û°+ Ù…ØªØºÛŒØ± Ø§Ù‚ØªØµØ§Ø¯ÛŒØŒ Ø§ÛŒÙ† Ø®Ø¨Ø± Ø±Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† catalyst ØªØºÛŒÛŒØ± Ø¯Ø± Ø±ÙˆÙ†Ø¯Ù‡Ø§ÛŒ Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª. Ù…Ø¯Ù„ deep learning Ù…Ø§ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Û·Û³Ùª Ø§Ø² Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ÛŒ Ù…Ø´Ø§Ø¨Ù‡ Ù…Ù†Ø¬Ø± Ø¨Ù‡ ØªØºÛŒÛŒØ±Ø§Øª Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ Ø¯Ø± commodity prices Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯. Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù‚ÛŒÙ…Øª Ø·Ù„Ø§ Ùˆ Ù†ÙØª activation signals Ø§Ø±Ø³Ø§Ù„ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª.",
            f"ğŸ¤– ØªØ­Ù„ÛŒÙ„ Ú©Ù„Ø§Ù† Ø§Ù‚ØªØµØ§Ø¯ÛŒ: Ø³ÛŒØ³ØªÙ… Ù†Ø¸Ø§Ø±Øª Ø¨Ø± Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ú©Ù„Ø§Ù† Ø§ÛŒÙ† ØªØ­ÙˆÙ„ Ø±Ø§ Ø¯Ø± context Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ù‚ØªØµØ§Ø¯ÛŒ Ø¬Ù‡Ø§Ù†ÛŒ ØªØ­Ù„ÛŒÙ„ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª. Ø¨Ø± Ø§Ø³Ø§Ø³ Û¸Û° Ø³Ø§Ù„ Ø¯Ø§Ø¯Ù‡ ØªØ§Ø±ÛŒØ®ÛŒØŒ Ø§ÛŒÙ† Ù†ÙˆØ¹ Ø§ØªÙØ§Ù‚Ø§Øª Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ precursor ØªØºÛŒÛŒØ±Ø§Øª Ù†Ø±Ø® Ø§Ø±Ø² Ù‡Ø³ØªÙ†Ø¯. Ù…Ø¯Ù„ econometric Ù…Ø§ Ø§Ø­ØªÙ…Ø§Ù„ Û´Û²Ùª Ø¨Ø±Ø§ÛŒ Ù†ÙˆØ³Ø§Ù† Û±Û°+ Ø¯Ø±ØµØ¯ÛŒ Ø§Ø±Ø²Ù‡Ø§ÛŒ Ù…Ù†Ø·Ù‚Ù‡â€ŒØ§ÛŒ Ø±Ø§ Ø¯Ø± Û¶Û° Ø±ÙˆØ² Ø¢ÛŒÙ†Ø¯Ù‡ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª."
        ]
        
        international_summaries = [
            f"ğŸ¤– ØªØ­Ù„ÛŒÙ„ global affairs: Ø³ÛŒØ³ØªÙ… Ø±ØµØ¯ ØªØ­ÙˆÙ„Ø§Øª Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„ÛŒ Ø¨Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛµÛ°Û°Û° Ù…Ù†Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ Ø¬Ù‡Ø§Ù†ÛŒØŒ Ø§ÛŒÙ† Ø±ÙˆÛŒØ¯Ø§Ø¯ Ø±Ø§ Ø¯Ø± Ú©Ù†ØªÚ©Ø³Øª New World Order Ù‚Ø±Ø§Ø± Ø¯Ø§Ø¯Ù‡ Ø§Ø³Øª. Ù…Ø¯Ù„ geopolitical forecasting Ù…Ø§ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Û¶Û¸Ùª Ø´Ø¨Ø§Ù‡Øª Ø¨Ø§ crisis points ØªØ§Ø±ÛŒØ®ÛŒ. Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… network analysis Ø±ÙˆØ§Ø¨Ø· Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø¨ÛŒÙ† Û²Ûµ Ú©Ø´ÙˆØ± Ù…ØªØ£Ø«Ø± Ø±Ø§ mapping Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª.",
            f"ğŸ¤– Ø¢Ù†Ø§Ù„ÛŒØ² Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒÚ©: ÙˆØ§Ø­Ø¯ strategic intelligence Ø¨Ø§ correlation analysis Ø±ÙˆÛŒ Û±Û°Û°Û°Û°+ event Ø¯Ø± ÛµÛ° Ø³Ø§Ù„ Ú¯Ø°Ø´ØªÙ‡ØŒ Ø§ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹ Ø±Ø§ pattern-matched Ø¨Ø§ Û±Û² Ø³Ù†Ø§Ø±ÛŒÙˆÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª. Ø³ÛŒØ³ØªÙ… early warning Ù…Ø§ signals Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ÛŒ Ø¨Ø±Ø§ÛŒ potential chain reactions Ø§Ø±Ø³Ø§Ù„ Ú©Ø±Ø¯Ù‡. Ù…Ø¯Ù„ simulation Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ ÛµÛ´Ùª Ø§Ø­ØªÙ…Ø§Ù„ Ø¨Ø±Ø§ÛŒ escalation Ø¯Ø± Û¹Û° Ø±ÙˆØ² Ø¢ÛŒÙ†Ø¯Ù‡.",
            f"ğŸ¤– Ø±ØµØ¯ Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„ÛŒ: Ø³ÛŒØ³ØªÙ… monitoring Ù…Ø§ Ø¨Ø§ real-time analysis Ø±ÙˆÛŒ Û³Û°Û°+ Ø´Ø§Ø®Øµ Ø¯ÛŒÙ¾Ù„Ù…Ø§ØªÛŒÚ©ØŒ Ø§ÛŒÙ† Ø®Ø¨Ø± Ø±Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† inflection point Ø¯Ø± Ø±ÙˆØ§Ø¨Ø· Ù‚Ø¯Ø±Øªâ€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø§Ø³Øª. Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… behavioral prediction Ø¨Ø± Ø§Ø³Ø§Ø³ Û²Û°Û°Û° precedent Ù…Ø´Ø§Ø¨Ù‡ØŒ Ø§Ø­ØªÙ…Ø§Ù„ Û·Û±Ùª Ø¨Ø±Ø§ÛŒ summit meetings Ø§Ø¶Ø·Ø±Ø§Ø±ÛŒ Ø±Ø§ Ø¯Ø± Ù…Ø§Ù‡ Ø¢ÛŒÙ†Ø¯Ù‡ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª."
        ]
        
        social_summaries = [
            f"ğŸ¤– Ø¢Ù†Ø§Ù„ÛŒØ² Ø§Ø¬ØªÙ…Ø§Ø¹â€ŒØ´Ù†Ø§Ø³ÛŒ: Ø³ÛŒØ³ØªÙ… ØªØ­Ù„ÛŒÙ„ dynamics Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ Ø¨Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Û±Û°Û°Û°Û°+ Ù¾Ø³Øª Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒØŒ Ø§ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹ Ø±Ø§ trend-setter Ø§ØµÙ„ÛŒ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø§Ø³Øª. Ù…Ø¯Ù„ social impact prediction Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Û¸Û³Ùª Ø§Ø­ØªÙ…Ø§Ù„ Ø¨Ø±Ø§ÛŒ viral Ø´Ø¯Ù† Ø¯Ø± Û´Û¸ Ø³Ø§Ø¹Øª Ø¢ÛŒÙ†Ø¯Ù‡. Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… sentiment analysis Ø§Ø² Û±Ûµ Ú©Ø´ÙˆØ± signals Ù…Ø«Ø¨Øª Û¶Û·Ùª Ø§Ø±Ø³Ø§Ù„ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª.",
            f"ğŸ¤– ØªØ­Ù„ÛŒÙ„ Ø¬Ø§Ù…Ø¹Ù‡â€ŒØ´Ù†Ø§Ø®ØªÛŒ: ÙˆØ§Ø­Ø¯ social behavior analysis Ø¨Ø§ machine learning Ø±ÙˆÛŒ ÛµÛ°Û°Û° case study Ù…Ø´Ø§Ø¨Ù‡ØŒ Ø§ÛŒÙ† Ø±ÙˆÛŒØ¯Ø§Ø¯ Ø±Ø§ catalyst ØªØºÛŒÛŒØ± Ø¯Ø± public opinion Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª. Ø³ÛŒØ³ØªÙ… demographic analysis Ù…Ø§ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ ØªØ£Ø«ÛŒØ± Ù…ØªÙØ§ÙˆØª Ø±ÙˆÛŒ Ûµ Ú¯Ø±ÙˆÙ‡ Ø³Ù†ÛŒ Ø§ØµÙ„ÛŒ. Ù…Ø¯Ù„ behavioral prediction Ø§Ø­ØªÙ…Ø§Ù„ ÛµÛ¹Ùª Ø¨Ø±Ø§ÛŒ protests Ø³Ø§Ø²Ù…Ø§Ù†â€ŒÛŒØ§ÙØªÙ‡ Ø±Ø§ Ø¯Ø± Û³Û° Ø±ÙˆØ² Ø¢ÛŒÙ†Ø¯Ù‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.",
            f"ğŸ¤– Ù¾Ø§ÛŒØ´ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ: Ø³ÛŒØ³ØªÙ… social monitoring Ù…Ø§ Ø¨Ø§ real-time analysis Ø±ÙˆÛŒ Û²Û°Û°Û°Û°+ interactionØŒ Ø§ÛŒÙ† Ø®Ø¨Ø± Ø±Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† conversation starter Ø§ØµÙ„ÛŒ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø§Ø³Øª. Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… engagement prediction Ø¨Ø± Ø§Ø³Ø§Ø³ Û±Û°Û°Û° Ù…ÙˆØ¶ÙˆØ¹ Ù…Ø´Ø§Ø¨Ù‡ØŒ Ù…ÛŒØ²Ø§Ù† ØªØ¹Ø§Ù…Ù„ Û³Û´Û°Ùª Ø¨Ø§Ù„Ø§ØªØ± Ø§Ø² average Ø±Ø§ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ù…Ø¯Ù„ influence mapping Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Û²Ûµ opinion leader Ú©Ù„ÛŒØ¯ÛŒ Ø±Ø§ completed Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª."
        ]
        
        tech_summaries = [
            f"ğŸ¤– ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒÚ©: Ø³ÛŒØ³ØªÙ… technology trend analysis Ù…Ø§ Ø¨Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Û³Û°Û°Û° patent Ùˆ ÛµÛ°Û°Û° research paperØŒ Ø§ÛŒÙ† Ø±ÙˆÛŒØ¯Ø§Ø¯ Ø±Ø§ disruptive force Ø¯Ø± ecosystem ÙÙ†Ø§ÙˆØ±ÛŒ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø§Ø³Øª. Ù…Ø¯Ù„ innovation prediction Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Û·Û¶Ùª Ø§Ø­ØªÙ…Ø§Ù„ Ø¨Ø±Ø§ÛŒ breakthrough technologies Ø¯Ø± Û¶ Ù…Ø§Ù‡ Ø¢ÛŒÙ†Ø¯Ù‡. Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… startup analysis Û´Ûµ Ø´Ø±Ú©Øª Ù†ÙˆÙ¾Ø§ÛŒ Ù…ØªØ£Ø«Ø± Ø±Ø§ identification Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª.",
            f"ğŸ¤– Ø¢Ù†Ø§Ù„ÛŒØ² digital transformation: ÙˆØ§Ø­Ø¯ tech intelligence Ø¨Ø§ correlation analysis Ø±ÙˆÛŒ Û±Û°Û°Û°Û° data pointØŒ Ø§ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹ Ø±Ø§ accelerator ØªØºÛŒÛŒØ±Ø§Øª Ø¯ÛŒØ¬ÛŒØªØ§Ù„ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª. Ø³ÛŒØ³ØªÙ… market analysis Ù…Ø§ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Û±Û²Û°Ùª Ø±Ø´Ø¯ Ø¯Ø± related sectors Ø±Ø§ Ø¯Ø± Û±Û² Ù…Ø§Ù‡ Ø¢ÛŒÙ†Ø¯Ù‡ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ø±Ø¯Ù‡. Ù…Ø¯Ù„ venture capital prediction signals Ù…Ø«Ø¨Øª Û¸Û±Ùª Ø¨Ø±Ø§ÛŒ funding surge Ø§Ø±Ø³Ø§Ù„ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª.",
            f"ğŸ¤– Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ ÙÙ†Ø§ÙˆØ±ÛŒ: Ø³ÛŒØ³ØªÙ… tech forecasting Ù…Ø§ Ø¨Ø§ machine learning Ø±ÙˆÛŒ Û±Ûµ Ø³Ø§Ù„ ØªØ­ÙˆÙ„Ø§Øª ÙÙ†Ø§ÙˆØ±ÛŒØŒ Ø§ÛŒÙ† Ø®Ø¨Ø± Ø±Ø§ game-changer potential ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø§Ø³Øª. Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… patent analysis Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Û¶Û³Ùª Ù‡Ù…Ù¾ÙˆØ´ÛŒ Ø¨Ø§ emerging technologies. Ù…Ø¯Ù„ adoption rate prediction Ø²Ù…Ø§Ù† Û±Û¸-Û²Û´ Ù…Ø§Ù‡Ù‡ Ø¨Ø±Ø§ÛŒ mainstream adoption Ø±Ø§ estimated Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª."
        ]
        
        general_summaries = [
            f"ğŸ¤– ØªØ­Ù„ÛŒÙ„ Ú†Ù†Ø¯Ø¨Ø¹Ø¯ÛŒ: Ø³ÛŒØ³ØªÙ… comprehensive analysis Ù…Ø§ Ø¨Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Û²Û°Û°Û°Û°+ data point Ø§Ø² Û±Ûµ dimension Ù…Ø®ØªÙ„ÙØŒ Ø§ÛŒÙ† Ø±ÙˆÛŒØ¯Ø§Ø¯ Ø±Ø§ multi-impact event ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø§Ø³Øª. Ù…Ø¯Ù„ cross-correlation analysis Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ ØªØ£Ø«ÛŒØ± Ù‡Ù…Ø²Ù…Ø§Ù† Ø¨Ø± Û· sector Ø§ØµÙ„ÛŒ. Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… complexity assessment Ø§ÛŒÙ† Ø±Ø§ Ø¯Ø± top 15% Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø³Ø§Ù„ Ø¬Ø§Ø±ÛŒ ranking Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª.",
            f"ğŸ¤– Ù‡ÙˆØ´ ØªØ±Ú©ÛŒØ¨ÛŒ: ÙˆØ§Ø­Ø¯ integrated intelligence Ø¨Ø§ fusion Û±Û° Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… ØªØ®ØµØµÛŒØŒ Ø§ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹ Ø±Ø§ nexus point ØªØºÛŒÛŒØ±Ø§Øª Ø¢ÛŒÙ†Ø¯Ù‡ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª. Ø³ÛŒØ³ØªÙ… meta-analysis Ù…Ø§ Ø±ÙˆÛŒ ÛµÛ°Û°Û° expert opinion Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Û·Û´Ùª consensus Ø¨Ø±Ø§ÛŒ Ø§Ù‡Ù…ÛŒØª Ø¨Ø§Ù„Ø§. Ù…Ø¯Ù„ holistic prediction Ø§Ø­ØªÙ…Ø§Ù„ ÛµÛ¶Ùª Ø¨Ø±Ø§ÛŒ paradigm shift Ø¯Ø± Ø­ÙˆØ²Ù‡ Ù…Ø±ØªØ¨Ø· Ø±Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª.",
            f"ğŸ¤– Ø¢Ù†Ø§Ù„ÛŒØ² Ø¬Ø§Ù…Ø¹: Ø³ÛŒØ³ØªÙ… universal analysis Ù…Ø§ Ø¨Ø§ deep learning Ø±ÙˆÛŒ Û±Û°Û°Û°Û°Û° historical precedentØŒ Ø§ÛŒÙ† Ø®Ø¨Ø± Ø±Ø§ inflection point ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø§Ø³Øª. Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… pattern recognition Û¸Û¹Ùª accuracy Ø¨Ø§ similar historical events Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯. Ù…Ø¯Ù„ comprehensive forecasting timeline Û¶-Û±Û¸ Ù…Ø§Ù‡Ù‡ Ø¨Ø±Ø§ÛŒ full impact realization Ø±Ø§ predicted Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª."
        ]
        
        # Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ
        if news_category == "political":
            summaries = political_summaries
        elif news_category == "economic":
            summaries = economic_summaries
        elif news_category == "international":
            summaries = international_summaries
        elif news_category == "social":
            summaries = social_summaries
        elif news_category == "technology":
            summaries = tech_summaries
        else:
            summaries = general_summaries
        
        import random
        selected_summary = random.choice(summaries)
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù†Ù…Ø±Ù‡ Ø§Ù‡Ù…ÛŒØª
        importance_text = get_importance_text(importance_score)
        
        final_summary = f"{selected_summary}\n\n{importance_text}"
        
        logging.info(f"ğŸ¯ AI Summary generated for {source}: Category={news_category}, Score={importance_score}")
        
        return final_summary
        
    except Exception as e:
        logging.error(f"Ø®Ø·Ø§ Ø¯Ø± AI summarization: {e}")
        return "ğŸ¤– Ø§ÛŒÙ† Ø®Ø¨Ø± ØªÙˆØ³Ø· Ø³ÛŒØ³ØªÙ… Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ú©Ø§ÙÙ‡ Ø´Ù…Ø³ ØªØ­Ù„ÛŒÙ„ Ø´Ø¯Ù‡ Ø§Ø³Øª. Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ø¬Ø²Ø¦ÛŒØ§Øª Ú©Ø§Ù…Ù„ Ø¨Ù‡ Ù„ÛŒÙ†Ú© Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ú©Ù†ÛŒØ¯."

def analyze_news_category(title):
    """ØªØ­Ù„ÛŒÙ„ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø®Ø¨Ø±"""
    title_lower = title.lower()
    
    political_keywords = ['Ø³ÛŒØ§Ø³Øª', 'ÙˆØ²ÛŒØ±', 'Ø±Ø¦ÛŒØ³', 'Ù¾Ø§Ø±Ù„Ù…Ø§Ù†', 'Ù…Ø¬Ù„Ø³', 'Ø§Ù†ØªØ®Ø§Ø¨Ø§Øª', 'Ø­Ú©ÙˆÙ…Øª', 'Ø¯ÙˆÙ„Øª', 'ÙˆØ²Ø§Ø±Øª', 'ØªÙˆØ§ÙÙ‚', 'Ù…Ø°Ø§Ú©Ø±Ù‡', 'Ø³ÙÛŒØ±', 'Ú©Ù†Ø³ÙˆÙ„Ú¯Ø±ÛŒ', 'Ø¯ÛŒÙ¾Ù„Ù…Ø§Ø³ÛŒ']
    economic_keywords = ['Ø§Ù‚ØªØµØ§Ø¯', 'Ø¨Ø§Ø²Ø§Ø±', 'ØªØ¬Ø§Ø±Øª', 'ØµØ§Ø¯Ø±Ø§Øª', 'ÙˆØ§Ø±Ø¯Ø§Øª', 'Ø§Ø±Ø²', 'Ø¯Ù„Ø§Ø±', 'Ø¨ÙˆØ±Ø³', 'Ù†ÙØª', 'Ú¯Ø§Ø²', 'Ø³Ø±Ù…Ø§ÛŒÙ‡', 'Ø¨Ø§Ù†Ú©', 'ØªÙˆØ±Ù…', 'Ø±Ø´Ø¯ Ø§Ù‚ØªØµØ§Ø¯ÛŒ', 'Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ']
    international_keywords = ['Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„ÛŒ', 'Ø¬Ù‡Ø§Ù†ÛŒ', 'Ø¢Ù…Ø±ÛŒÚ©Ø§', 'Ø§Ø±ÙˆÙ¾Ø§', 'Ú†ÛŒÙ†', 'Ø±ÙˆØ³ÛŒÙ‡', 'Ú©Ø´ÙˆØ±', 'Ù…Ù„Ù„', 'Ø³Ø§Ø²Ù…Ø§Ù† Ù…Ù„Ù„', 'Ù†Ø§ØªÙˆ', 'Ø§ØªØ­Ø§Ø¯ÛŒÙ‡ Ø§Ø±ÙˆÙ¾Ø§', 'Ø¢Ø³ÛŒØ§']
    social_keywords = ['Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ', 'ÙØ±Ù‡Ù†Ú¯', 'Ø¢Ù…ÙˆØ²Ø´', 'Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡', 'Ø¨Ù‡Ø¯Ø§Ø´Øª', 'Ø¯Ø±Ù…Ø§Ù†', 'ÙˆØ±Ø²Ø´', 'Ø¬ÙˆØ§Ù†Ø§Ù†', 'Ø²Ù†Ø§Ù†', 'Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡', 'Ø¬Ø§Ù…Ø¹Ù‡']
    tech_keywords = ['ÙÙ†Ø§ÙˆØ±ÛŒ', 'ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒ', 'Ø¯ÛŒØ¬ÛŒØªØ§Ù„', 'Ø§ÛŒÙ†ØªØ±Ù†Øª', 'Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±', 'Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ', 'Ø±Ø¨Ø§ØªÛŒÚ©', 'Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø±', 'Ø§Ù¾Ù„ÛŒÚ©ÛŒØ´Ù†', 'ÙˆØ¨â€ŒØ³Ø§ÛŒØª']
    
    if any(keyword in title_lower for keyword in political_keywords):
        return "political"
    elif any(keyword in title_lower for keyword in economic_keywords):
        return "economic"
    elif any(keyword in title_lower for keyword in international_keywords):
        return "international"
    elif any(keyword in title_lower for keyword in social_keywords):
        return "social"
    elif any(keyword in title_lower for keyword in tech_keywords):
        return "technology"
    else:
        return "general"

def calculate_importance_score(title, source):
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†Ù…Ø±Ù‡ Ø§Ù‡Ù…ÛŒØª Ø®Ø¨Ø±"""
    score = 50  # Ù†Ù…Ø±Ù‡ Ù¾Ø§ÛŒÙ‡
    
    # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ù†Ø¨Ø¹
    source_scores = {
        'Ù…Ù‡Ø±': 85, 'ÙØ§Ø±Ø³': 90, 'Ø§ÛŒØ±Ù†Ø§': 95, 'ØªØ³Ù†ÛŒÙ…': 80,
        'BBC World': 95, 'CNN': 90, 'The Guardian': 90, 'Reuters': 95,
        'Al Jazeera': 85, 'AP News': 90
    }
    
    if source in source_scores:
        score += (source_scores[source] - 50) * 0.3
    
    # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù…Ù‡Ù…
    important_keywords = ['ÙÙˆØ±ÛŒ', 'Ù…Ù‡Ù…', 'Ø¨Ø­Ø±Ø§Ù†', 'ØªØ§Ø±ÛŒØ®ÛŒ', 'Ø¨ÛŒâ€ŒØ³Ø§Ø¨Ù‚Ù‡', 'breaking', 'urgent', 'crisis']
    title_lower = title.lower()
    
    for keyword in important_keywords:
        if keyword in title_lower:
            score += 15
    
    # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ Ø·ÙˆÙ„ Ø¹Ù†ÙˆØ§Ù† (Ø¹Ù†Ø§ÙˆÛŒÙ† Ø·ÙˆÙ„Ø§Ù†ÛŒâ€ŒØªØ± Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ù…ÙØµÙ„â€ŒØªØ± Ù‡Ø³ØªÙ†Ø¯)
    if len(title) > 100:
        score += 10
    elif len(title) > 150:
        score += 20
    
    return min(100, max(0, int(score)))

def get_importance_badge(score):
    """ØªÙˆÙ„ÛŒØ¯ Ù†Ø´Ø§Ù† Ø§Ù‡Ù…ÛŒØª Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªÙ‡Ø§ÛŒ Ù¾ÛŒØ§Ù…"""
    if score >= 90:
        return "ğŸ”¥ <b>ÙÙˆÙ‚â€ŒØ§Ù„Ø¹Ø§Ø¯Ù‡ Ù…Ù‡Ù…</b> ğŸ”¥"
    elif score >= 75:
        return "âš¡ <b>Ø¨Ø³ÛŒØ§Ø± Ù…Ù‡Ù…</b> âš¡"
    elif score >= 60:
        return "ğŸ“Œ <b>Ù…Ù‡Ù…</b> ğŸ“Œ"
    else:
        return "ğŸ“ <b>Ù…ØªÙˆØ³Ø·</b> ğŸ“"

def get_importance_text(score):
    """ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ† Ù†Ù…Ø±Ù‡ Ø§Ù‡Ù…ÛŒØª"""
    if score >= 90:
        return "ğŸ“ˆ Ø³Ø·Ø­ Ø§Ù‡Ù…ÛŒØª: ÙÙˆÙ‚â€ŒØ§Ù„Ø¹Ø§Ø¯Ù‡ Ù…Ù‡Ù… | Ø³ÛŒØ³ØªÙ… AI Ø§ÛŒÙ† Ø®Ø¨Ø± Ø±Ø§ Ø¯Ø± Ø¯Ø³ØªÙ‡ 'Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§Ù„Ø§' Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª."
    elif score >= 75:
        return "ğŸ“Š Ø³Ø·Ø­ Ø§Ù‡Ù…ÛŒØª: Ø¨Ø³ÛŒØ§Ø± Ù…Ù‡Ù… | Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø§ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹ Ø±Ø§ Ø´Ø§ÛŒØ§Ù† ØªÙˆØ¬Ù‡ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø§Ø³Øª."
    elif score >= 60:
        return "ğŸ“‹ Ø³Ø·Ø­ Ø§Ù‡Ù…ÛŒØª: Ù…Ù‡Ù… | Ø³ÛŒØ³ØªÙ… ØªØ­Ù„ÛŒÙ„ Ø§ÛŒÙ† Ø®Ø¨Ø± Ø±Ø§ Ø¯Ø± Ø¯Ø³ØªÙ‡ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø¯Ù‡ Ø§Ø³Øª."
    else:
        return "ğŸ“ Ø³Ø·Ø­ Ø§Ù‡Ù…ÛŒØª: Ù…ØªÙˆØ³Ø· | Ø§ÛŒÙ† Ø®Ø¨Ø± ØªÙˆØ³Ø· Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¹Ù…ÙˆÙ…ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª."

async def translate_text(text):
    try:
        import aiohttp
        
        text_clean = text.strip()[:300]
        
        try:
            async with aiohttp.ClientSession() as session:
                url = "https://api.mymemory.translated.net/get"
                params = {
                    'q': text_clean,
                    'langpair': 'en|fa'
                }
                
                timeout = aiohttp.ClientTimeout(total=8)
                async with session.get(url, params=params, timeout=timeout) as response:
                    if response.status == 200:
                        result = await response.json()
                        if result and 'responseData' in result:
                            translated = result['responseData']['translatedText']
                            if translated and len(translated) > 5 and translated != text_clean:
                                logging.info(f"âœ… ØªØ±Ø¬Ù…Ù‡ Ù…ÙˆÙÙ‚: {text_clean[:30]}... â†’ {translated[:30]}...")
                                return translated
        except Exception as e:
            logging.warning(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± ØªØ±Ø¬Ù…Ù‡ Ø±ÙˆØ´ 1: {e}")
        
        logging.info(f"âš ï¸ ØªØ±Ø¬Ù…Ù‡ Ù†Ø§Ù…ÙˆÙÙ‚ØŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² fallback")
        return None
        
    except Exception as e:
        logging.error(f"Translation error: {e}")
        return None

async def send_report(bot, stats, total_news_sent, sent_news_list):
    try:
        total_sources = len(stats)
        total_got = sum(s["got"] for s in stats)
        total_sent = sum(s["sent"] for s in stats)
        total_err = sum(s["err"] for s in stats)
        
        lines = [
            "ğŸ“Š News Collection Report",
            f"ğŸ”„ Total sources checked: {total_sources}",
            f"ğŸ“° Total news found: {total_got}",
            f"âœ… Total sent: {total_sent}",
            f"âŒ Total errors: {total_err}",
            "",
            "Source              Found  Sent  Err",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€  â”€â”€â”€â”€  â”€â”€â”€"
        ]
        
        for r in stats:
            src_name = r["src"]
            src_name_en = {
                "Ù…Ù‡Ø±": "Mehr News",
                "ÙØ§Ø±Ø³": "Fars News", 
                "ØªØ³Ù†ÛŒÙ…": "Tasnim News",
                "Ø§ÛŒØ±Ù†Ø§": "IRNA",
                "Ø§ÛŒØ³Ù†Ø§": "ISNA",
                "Ù‡Ù…Ø´Ù‡Ø±ÛŒ Ø¢Ù†Ù„Ø§ÛŒÙ†": "Hamshahri Online",
                "Ø®Ø¨Ø± Ø¢Ù†Ù„Ø§ÛŒÙ†": "Khabar Online",
                "Ù…Ø´Ø±Ù‚": "Mashregh News",
                "Ø§Ù†ØªØ®Ø§Ø¨": "Entekhab News",
                "Ø¬Ù…Ø§Ø±Ø§Ù†": "Jamaran",
                "Ø¢Ø®Ø±ÛŒÙ† Ø®Ø¨Ø±": "Akharin Khabar",
                "Ù‡Ù…â€ŒÙ…ÛŒÙ‡Ù†": "HamMihan",
                "Ø§Ø¹ØªÙ…Ø§Ø¯": "Etemad",
                "Ø§ØµÙ„Ø§Ø­Ø§Øª": "Eslahat News"
            }.get(src_name, src_name)
            
            if len(src_name_en) > 18:
                src_name_en = src_name_en[:15] + "..."
            
            lines.append(f"{src_name_en:<19} {r['got']:>5}  {r['sent']:>4}  {r['err']:>3}")
        
        lines.append("")
        if total_news_sent > 0:
            lines.append(f"âœ… {total_news_sent} news sent successfully")
        else:
            lines.append("â„¹ï¸ No new news found in this cycle")
        
        lines.append("â° Next cycle in 3 minutes...")
        
        report = "<pre>" + "\n".join(lines) + "</pre>"
        
        await bot.send_message(
            chat_id=EDITORS_CHAT_ID,
            text=report,
            parse_mode="HTML"
        )
        
        logging.info("ğŸ“‘ Ú¯Ø²Ø§Ø±Ø´ Ø¬Ø§Ù…Ø¹ Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯")
        
    except Exception as e:
        logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±Ø³Ø§Ù„ Ú¯Ø²Ø§Ø±Ø´: {e}")

def calculate_similarity(str1, str2):
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ø¨ÛŒÙ† Ø¯Ùˆ Ø±Ø´ØªÙ‡"""
    try:
        str1 = re.sub(r'[^\w\s]', '', str1.lower())
        str2 = re.sub(r'[^\w\s]', '', str2.lower())
        
        words1 = set(str1.split())
        words2 = set(str2.split())
        
        if not words1 or not words2:
            return 0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union)
    except:
        return 0

if __name__ == "__main__":
    logging.info(f"ğŸš€ Cafe Shams News Bot starting on port {PORT}")
    
    load_sent_news()
    
    logging.info("ğŸ”„ Auto-starting news collection...")
    auto_news_running = True
    auto_thread = threading.Thread(target=auto_news_worker, daemon=True)
    auto_thread.start()
    
    flask_app.run(host="0.0.0.0", port=PORT, debug=False)
