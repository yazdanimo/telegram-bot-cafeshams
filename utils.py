import os
import sys
import json
import re
import logging
from bs4 import BeautifulSoup

BASE_DIR     = os.path.dirname(__file__)
SOURCES_PATH = os.path.join(BASE_DIR, "sources.json")

# ðŸ“¥ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ù†Ø§Ø¨Ø¹
def load_sources():
    if not os.path.exists(SOURCES_PATH):
        sys.exit(f"ERROR: sources.json not found at {SOURCES_PATH}")
    try:
        with open(SOURCES_PATH, encoding="utf-8") as f:
            return json.load(f)
    except json.JSONDecodeError as e:
        sys.exit(f"ERROR: Invalid JSON in sources.json:\n  {e}")

# ðŸ“¥ Ø®ÙˆØ§Ù†Ø¯Ù† Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ
def load_set(path: str) -> set:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return set(json.load(f))
    except:
        return set()

# ðŸ’¾ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒÙ‡Ø§
def save_set(data: set, path: str):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(list(data), f, ensure_ascii=False, indent=2)

# ðŸ“„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ Ø®Ø¨Ø±
def extract_full_content(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")
    article = soup.find("article")
    if article:
        return article.get_text("\n").strip()
    return "\n".join(p.get_text() for p in soup.find_all("p")).strip()

# ðŸ§  Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ ÙØ§Ø±Ø³ÛŒ
def summarize_fa(text: str, max_s: int = 6) -> str:
    parts = re.split(r"[.ØŸ!]\s*", text)
    summary = [p.strip() for p in parts if p.strip()]
    return " ".join(summary[:max_s])

# ðŸ§  Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ
def summarize_en(text: str, max_s: int = 5) -> str:
    parts = re.split(r"[.?!]\s*", text)
    summary = [p.strip() for p in parts if p.strip()]
    return ". ".join(summary[:max_s])

# âœï¸ Ù‚Ø§Ù„Ø¨ Ù†Ù‡Ø§ÛŒÛŒ Ø®Ø¨Ø± Ø¨Ø±Ø§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù…
def format_news(source: str, title: str, summary: str, link: str) -> str:
    clean_summary = summary.replace("\n", " ").strip()
    return (
        f"ðŸ“° {source}\n\n"
        f"**{title.strip()}**\n\n"
        f"{clean_summary}\n\n"
        f"ðŸ”— [Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ú©Ø§Ù…Ù„ Ø®Ø¨Ø±]({link})\n"
        f"ðŸ†” @cafeshamss â€” Ú©Ø§ÙÙ‡ Ø´Ù…Ø³ â˜•ï¸ðŸª"
    )

# ðŸ§¹ ÙÛŒÙ„ØªØ± ØµÙØ­Ø§Øª Ø¨ÛŒâ€ŒØ§Ø±Ø²Ø´
def is_garbage(text: str) -> bool:
    t = text.strip()
    if len(t) < 40:
        return True
    lower = t.lower()
    for kw in ["Ø«Ø¨Øª Ù†Ø§Ù…", "ÙˆØ±ÙˆØ¯", "login", "register", "signup", "Ø±Ù…Ø² Ø¹Ø¨ÙˆØ±"]:
        if kw in lower:
            return True
    return False

# âœ… Ø§Ø±Ø³Ø§Ù„ Ø§ÛŒÙ…Ù† Ù¾ÛŒØ§Ù…
async def safe_send(bot, chat_id, text, **kwargs):
    try:
        await bot.send_message(chat_id=chat_id, text=text, **kwargs)
    except Exception as e:
        logging.warning(f"â—ï¸ Failed to send message: {e}")
