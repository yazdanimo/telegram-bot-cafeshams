# File: fetch_news.py â€” Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø¨Ø§ ÙÛŒÙ„ØªØ± Ù‚ÙˆÛŒ Ùˆ Ù…Ø¯ÛŒØ±ÛŒØª fallback

# âš™ï¸ Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§ Ùˆ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
import aiohttp, asyncio, json, time, hashlib, feedparser, re
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urlunparse, parse_qsl
from utils import load_sources, extract_full_content, summarize_text, format_news

# ğŸ“ Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ùˆ ÙØ§ØµÙ„Ù‡ Ø§Ø±Ø³Ø§Ù„
SEND_INTERVAL      = 3
LAST_SEND          = 0
SENT_URLS_FILE     = "sent_urls.json"
SENT_HASHES_FILE   = "sent_hashes.json"
BAD_LINKS_FILE     = "bad_links.json"
SKIPPED_LOG_FILE   = "skipped_items.json"
GARBAGE_NEWS_FILE  = "garbage_news.json"

# ğŸ“¦ Ø§Ø¨Ø²Ø§Ø± ÙØ§ÛŒÙ„ Ùˆ URL
def load_set(path):
    try: with open(path, "r", encoding="utf-8") as f: return set(json.load(f))
    except: return set()

def save_set(data, path):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(list(data), f, ensure_ascii=False, indent=2)

def normalize_url(url):
    p = urlparse(url)
    qs = [(k,v) for k,v in parse_qsl(p.query) if not k.startswith("utm_")]
    return urlunparse((p.scheme,p.netloc,p.path,"","&".join(f"{k}={v}" for k,v in qs),""))

# ğŸ§  ÙÛŒÙ„ØªØ± Ù…Ø­ØªÙˆØ§ÛŒ Ø¨ÛŒâ€ŒØ±Ø¨Ø· ÛŒØ§ Ø®Ø±Ø§Ø¨
def is_garbage(text):
    text = text.strip()
    if len(text) < 60: return True
    persian = re.findall(r'[\u0600-\u06FF]', text)
    if len(persian)/max(len(text),1) < 0.4: return True
    if re.search(r'(.)\1{5,}', text): return True
    if len(re.findall(r'[A-Za-z]{4,}', text)) > 5 and len(persian) < 50: return True
    return False

# ğŸ“ Ø«Ø¨Øª Ù…Ø­ØªÙˆØ§ÛŒ Ø®Ø±Ø§Ø¨ Ùˆ Ø±Ø¯Ø´Ø¯Ù‡â€ŒÙ‡Ø§
def log_garbage(source, link, title, content):
    try: items = json.load(open(GARBAGE_NEWS_FILE, "r", encoding="utf-8"))
    except: items = []
    items.append({"source": source, "link": link, "title": title, "content": content[:300]})
    with open(GARBAGE_NEWS_FILE, "w", encoding="utf-8") as f:
        json.dump(items, f, ensure_ascii=False, indent=2)

def log_skipped(source, url, reason, title=None):
    try: items = json.load(open(SKIPPED_LOG_FILE, "r", encoding="utf-8"))
    except: items = []
    items.append({"source": source, "url": url, "title": title, "reason": reason})
    with open(SKIPPED_LOG_FILE, "w", encoding="utf-8") as f:
        json.dump(items, f, ensure_ascii=False, indent=2)

# ğŸ“¨ Ø§Ø±Ø³Ø§Ù„ Ø§Ù…Ù† Ù¾ÛŒØ§Ù…
async def safe_send(bot, chat_id, text, **kwargs):
    global LAST_SEND
    wait = SEND_INTERVAL - (time.time() - LAST_SEND)
    if wait > 0: await asyncio.sleep(wait)
    try:
        return await bot.send_message(chat_id=chat_id, text=text, **kwargs)
    except Exception as e:
        print("âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±Ø³Ø§Ù„:", e)
    finally:
        LAST_SEND = time.time()

# ğŸŒ Ø®ÙˆØ§Ù†Ø¯Ù† RSS
async def parse_rss_async(url):
    try:
        dp = await asyncio.wait_for(asyncio.to_thread(feedparser.parse, url), timeout=10)
        return dp.entries or []
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± RSS {url}:", e)
        return []

# ğŸŒ Ø¯Ø±ÛŒØ§ÙØª HTML
async def fetch_html(session, url):
    async with session.get(url) as res:
        if res.status != 200:
            raise Exception(f"HTTP {res.status}")
        return await res.text()

# ğŸ” ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø§Ø±Ø³Ø§Ù„ Ø®Ø¨Ø±
async def fetch_and_send_news(bot, chat_id, sent_urls, sent_hashes):
    bad_links  = load_set(BAD_LINKS_FILE)
    stats      = []
    sent_now   = set()
    hashes_now = set()

    async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=20)) as session:
        for src in load_sources():
            name, rss, fb = src["name"], src["rss"], src.get("fallback")
            sent = err = total = 0

            items = await parse_rss_async(rss)
            total = len(items)
            print(f"ğŸ“¥ Ø¯Ø±ÛŒØ§ÙØª {total} Ø¢ÛŒØªÙ… Ø§Ø² {name}")

            # ğŸ”¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ RSS
            for item in items[:3]:
                raw = item.get("link", "")
                u = normalize_url(raw)
                if not u or u in sent_urls or u in sent_now or u in bad_links:
                    log_skipped(name, u, "URL ØªÚ©Ø±Ø§Ø±ÛŒ", item.get("title")); continue
                try:
                    html = await fetch_html(session, raw)
                    full = extract_full_content(html)
                    summ = summarize_text(full)
                    if is_garbage(full) or is_garbage(summ):
                        log_skipped(name, u, "Ø¨ÛŒâ€ŒÚ©ÛŒÙÛŒØª", item.get("title"))
                        log_garbage(name, raw, item.get("title", ""), full)
                        bad_links.add(u); err += 1; continue
                    cap = format_news(name, item.get("title",""), summ, raw)
                    h   = hashlib.md5(cap.encode("utf-8")).hexdigest()
                    if h in sent_hashes or h in hashes_now:
                        log_skipped(name, u, "ØªÚ©Ø±Ø§Ø±ÛŒ", item.get("title")); continue
                    await safe_send(bot, chat_id, cap, parse_mode="HTML")
                    sent_now.add(u); hashes_now.add(h); sent += 1
                except Exception as e:
                    log_skipped(name, u, f"Ø®Ø·Ø§: {e}", item.get("title"))
                    print("âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´", raw, "â†’", e)
                    bad_links.add(u); err += 1

            # ğŸ”¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ fallback Ø§Ú¯Ø± RSS Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯
            if total == 0 and fb:
                try:
                    html_index = await fetch_html(session, fb)
                    soup = BeautifulSoup(html_index, "html.parser")
                    base = urlparse(fb)
                    links = []
                    for a in soup.find_all("a", href=True):
                        href = a["href"]
                        if href.startswith("/"):
                            href = urlunparse((base.scheme, base.netloc, href, "", "", ""))
                        netloc = urlparse(href).netloc
                        if netloc == base.netloc and href not in links:
                            links.append(href)
                        if len(links) >= 3: break

                    for link in links:
                        u = normalize_url(link)
                        if not u or u in sent_urls or u in sent_now or u in bad_links:
                            log_skipped(name, u, "fallback ØªÚ©Ø±Ø§Ø±ÛŒ", "fallback"); continue
                        try:
                            html = await fetch_html(session, link)
                            full = extract_full_content(html)
                            summ = summarize_text(full)
                            if is_garbage(full) or is_garbage(summ):
                                log_skipped(name, u, "fallback Ø¨ÛŒâ€ŒÚ©ÛŒÙÛŒØª", "fallback")
                                log_garbage(name, link, "fallback", full)
                                bad_links.add(u); err += 1; continue
                            cap = format_news(f"{name} - fallback", "fallback", summ, link)
                            h   = hashlib.md5(cap.encode("utf-8")).hexdigest()
                            if h in sent_hashes or h in hashes_now:
                                log_skipped(name, u, "fallback ØªÚ©Ø±Ø§Ø±ÛŒ", "fallback"); continue
                            await safe_send(bot, chat_id, cap, parse_mode="HTML")
                            hashes_now.add(h); sent += 1
                        except Exception as fe:
                            log_skipped(name, link, f"Ø®Ø·Ø§ fallback: {fe}", "fallback")
                            print("âŒ Ø®Ø·Ø§ Ø¯Ø± fallback", name, "â†’", fe)
                            bad_links.add(u); err += 1
                except Exception as e:
                    log_skipped(name, fb, f"fallback index error: {e}")
                    print("âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª fallback:", e)
                    bad_links.add(fb); err += 1

            stats.append({"Ù…Ù†Ø¨Ø¹": name, "Ø¯Ø±ÛŒØ§ÙØª": total, "Ø§Ø±Ø³Ø§Ù„": sent, "Ø®Ø·Ø§": err})

        # ğŸ“ Ø°Ø®ÛŒØ±Ù‡ ÙˆØ¶Ø¹ÛŒØª
        sent_urls.update(sent_now); sent_hash
