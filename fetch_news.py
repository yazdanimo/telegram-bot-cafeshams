import requests
from bs4 import BeautifulSoup
from langdetect import detect
from translatepy import Translator
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer
from utils import extract_full_content, extract_image_from_html
import json
import nltk
import asyncio

# ğŸ§  Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ NLTK Ø¨Ø±Ø§ÛŒ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ
nltk.download("punkt")

translator = Translator()

# ğŸ“š Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ
with open("sources.json", "r", encoding="utf-8") as f:
    sources = json.load(f)

def summarize_text(text, sentence_count=4):
    try:
        parser = PlaintextParser.from_string(text, Tokenizer("english"))
        summary = LsaSummarizer()(parser.document, sentence_count)
        summarized = " ".join(str(sentence) for sentence in summary).strip()
        return summarized if len(summarized) > 100 else text[:400]
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ†: {e}")
        return text[:400]

async def fetch_and_send_news(bot, chat_id, sent_urls):
    for source in sources:
        name = source.get("name")
        url = source.get("url")

        try:
            rss = requests.get(url, timeout=10)
            rss.raise_for_status()
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª RSS Ø§Ø² {name}: {e}")
            continue

        soup = BeautifulSoup(rss.content, "xml")
        items = soup.find_all("item")
        print(f"\nğŸ“¡ Ø¯Ø±ÛŒØ§ÙØª RSS Ø§Ø² {name} â†’ Ù…Ø¬Ù…ÙˆØ¹: {len(items)}")

        for item in items[:3]:  # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Û³ Ø®Ø¨Ø± Ø¨Ø±Ø§ÛŒ Ú©Ù†ØªØ±Ù„ flood
            link = item.link.text.strip() if item.link else ""
            if not link or link in sent_urls:
                continue

            title = item.title.text.strip() if item.title else "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†"
            raw_html = item.description.text.strip() if item.description else ""

            image_url = extract_image_from_html(raw_html)
            full_text, _ = extract_full_content(link)

            # Ø±Ø¯ Ø®Ø¨Ø±Ù‡Ø§ÛŒÛŒ Ø¨Ø§ Ù…ØªÙ† Ù†Ø§Ù‚Øµ ÛŒØ§ ØºÛŒØ±Ø®Ø¨Ø±ÛŒ
            if not full_text or len(full_text.strip()) < 300:
                print(f"âš ï¸ Ø±Ø¯ Ø´Ø¯: Ù…Ø­ØªÙˆØ§ÛŒ Ø¶Ø¹ÛŒÙ ÛŒØ§ ØºÛŒØ±Ø®Ø¨Ø±ÛŒ Ø§Ø² {name}")
                continue
            if any(x in full_text for x in ["Languages", "Privacy Policy", "404", "Ú©Ø¯ Ø§Ø³ØªØ§ØªÙˆØ³", "ØµÙØ­Ù‡ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª"]):
                print(f"âš ï¸ Ø±Ø¯ Ø´Ø¯: Ù…Ø­ØªÙˆØ§ÛŒ Ù…Ø´Ú©ÙˆÚ© ÛŒØ§ Ø®Ø·Ø§ÛŒ HTML Ø§Ø² {name}")
                continue

            # ØªØ±Ø¬Ù…Ù‡ Ø§Ú¯Ø± Ø®Ø¨Ø± Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¨Ø§Ø´Ø¯
            try:
                lang = detect(title + full_text)
                if lang == "en":
                    title = translator.translate(title, "Persian").result
                    full_text = translator.translate(full_text, "Persian").result
            except Exception as e:
                print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± ØªØ±Ø¬Ù…Ù‡ ÛŒØ§ ØªØ´Ø®ÛŒØµ Ø²Ø¨Ø§Ù† Ø®Ø¨Ø± {name}: {e}")
                continue

            summary = summarize_text(full_text, 4)

            caption = (
                f"ğŸ“¡ Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ {name}\n"
                f"{title}\n\n"
                f"{summary.strip()}\n\n"
                f"ğŸ†” @cafeshamss\nÚ©Ø§ÙÙ‡ Ø´Ù…Ø³ â˜•ï¸ğŸª"
            )

            try:
                if image_url:
                    await bot.send_photo(chat_id=chat_id, photo=image_url, caption=caption[:1024])
                else:
                    await bot.send_message(chat_id=chat_id, text=caption[:4096])
                print(f"âœ… Ø®Ø¨Ø± Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯ Ø§Ø² {name}")
                sent_urls.add(link)
                await asyncio.sleep(2)  # ÙØ§ØµÙ„Ù‡ Û² Ø«Ø§Ù†ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² flood
            except Exception as e:
                print(f"â—ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±Ø³Ø§Ù„ Ø®Ø¨Ø± Ø§Ø² {name}: {e}")

    return sent_urls
