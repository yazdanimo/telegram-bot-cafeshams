import aiohttp
import asyncio
import time
import hashlib
import logging
import feedparser
import re
from urllib.parse import urlparse, urlunparse, parse_qsl
from bs4 import BeautifulSoup
from translatepy import Translator
from utils import (
    load_sources,
    extract_full_content,
    summarize_fa,
    summarize_en,
    format_news,
    load_set,
    save_set,
    is_garbage,
    safe_send
)
from handlers import send_news_with_button

SEND_INTERVAL = 10  # ÙØ§ØµÙ„Ù‡ Ø¨ÛŒÙ† Ø§Ø±Ø³Ø§Ù„ Ø§Ø®Ø¨Ø§Ø±
MAX_PER_CYCLE = 1   # ÙÙ‚Ø· ÛŒÚ© Ø®Ø¨Ø± Ø¯Ø± Ù‡Ø± Ø¯ÙˆØ± (ØªØ§ ØªÚ©Ø±Ø§Ø± Ù†Ø¨Ø§Ø´Ù‡)
FILES = {
    "urls": "sent_urls.json",
    "hashes": "sent_hashes.json",
    "bad": "bad_links.json"
}

translator = Translator()

def normalize_url(url: str) -> str:
    try:
        p = urlparse(url)
        qs = [(k, v) for k, v in parse_qsl(p.query) if not k.startswith("utm_")]
        return urlunparse((p.scheme, p.netloc, p.path, "", "&".join(f"{k}={v}" for k, v in qs), ""))
    except:
        return url

async def parse_rss(url: str):
    try:
        dp = await asyncio.wait_for(
            asyncio.to_thread(feedparser.parse, url),
            timeout=10
        )
        return dp.entries or []
    except asyncio.TimeoutError:
        logging.warning(f"RSS timeout: {url}")
        return []
    except Exception as e:
        logging.error(f"RSS parse error {url}: {e}")
        return []

async def fetch_html(session, url: str) -> str:
    try:
        async with session.get(url, timeout=5) as r:
            if r.status == 200:
                return await r.text()
    except asyncio.TimeoutError:
        logging.warning(f"HTML timeout: {url}")
    except Exception as e:
        logging.error(f"HTML fetch error {url}: {e}")
    return ""

async def safe_translate(text: str, target_lang: str = "fa") -> str:
    try:
        tr = await asyncio.wait_for(
            asyncio.to_thread(translator.translate, text, target_lang),
            timeout=10
        )
        return getattr(tr, "result", str(tr))
    except asyncio.TimeoutError:
        logging.warning("Translation timeout")
        return text
    except Exception as e:
        logging.error(f"Translation error: {e}")
        return text

async def fetch_and_send_news(bot, chat_id, sent_urls: set, sent_hashes: set):
    bad = load_set(FILES["bad"])
    stats = []
    new_urls = set()
    new_h = set()
    
    # Ù¾ÛŒØ§Ù… Ø´Ø±ÙˆØ¹ Ø¯ÙˆØ± Ø¬Ø¯ÛŒØ¯
    logging.info("ğŸ”„ Ø´Ø±ÙˆØ¹ Ø¯ÙˆØ± Ø¬Ø¯ÛŒØ¯ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø®Ø¨Ø§Ø±...")
    
    connector = aiohttp.TCPConnector(limit=10, limit_per_host=5)
    timeout = aiohttp.ClientTimeout(total=30)
    
    # Ù…ØªØºÛŒØ± Ø¨Ø±Ø§ÛŒ Ú©Ù†ØªØ±Ù„ Ø§Ø±Ø³Ø§Ù„ ÙÙ‚Ø· ÛŒÚ© Ø®Ø¨Ø± Ø¯Ø± Ù‡Ø± Ø¯ÙˆØ±
    news_sent_in_cycle = False
    
    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as sess:
        sources = load_sources()
        
        for src in sources:
            name, rss, fb, lang = src["name"], src["rss"], src["fallback"], src["lang"]
            got = sent = err = 0

            logging.info(f"ğŸ“¡ Ø¨Ø±Ø±Ø³ÛŒ Ù…Ù†Ø¨Ø¹: {name}")
            
            try:
                items = await parse_rss(rss)
                got = len(items)
                
                # Ø§Ú¯Ø± Ù‚Ø¨Ù„Ø§Ù‹ Ø¯Ø± Ø§ÛŒÙ† Ø¯ÙˆØ± Ø®Ø¨Ø±ÛŒ Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯Ù‡ØŒ ÙÙ‚Ø· Ø¢Ù…Ø§Ø± Ø¬Ù…Ø¹ Ú©Ù†
                if news_sent_in_cycle:
                    logging.info(f"â­ï¸ Ø±Ø¯ Ø´Ø¯: Ù‚Ø¨Ù„Ø§Ù‹ Ø¯Ø± Ø§ÛŒÙ† Ø¯ÙˆØ± Ø®Ø¨Ø± Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯Ù‡")
                    stats.append({"src": name, "got": got, "sent": 0, "err": 0})
                    continue
                
                for entry in items:
                    try:
                        link = entry.get("link", "").strip()
                        if not link:
                            continue
                            
                        u = normalize_url(link)
                        if not u or u in sent_urls | new_urls | bad:
                            continue

                        html = await fetch_html(sess, link)
                        if not html:
                            continue
                            
                        full = extract_full_content(html)
                        if is_garbage(full):
                            bad.add(u)
                            err += 1
                            continue

                        if lang == "en":
                            fa_text = await safe_translate(full, "fa")
                            summ = summarize_fa(fa_text)
                        else:
                            summ = summarize_fa(full)

                        if is_garbage(summ):
                            bad.add(u)
                            err += 1
                            continue

                        title = entry.get("title", "").strip() or name
                        text = format_news(name, title, summ, link)
                        h = hashlib.md5(text.encode()).hexdigest()
                        
                        if h in sent_hashes | new_h:
                            continue

                        logging.info(f"âœ… Ø§Ø±Ø³Ø§Ù„ Ø®Ø¨Ø± Ø§Ø² {name}: {title}")
                        await send_news_with_button(bot, chat_id, text)
                        new_urls.add(u)
                        new_h.add(h)
                        sent += 1
                        news_sent_in_cycle = True  # Ø¹Ù„Ø§Ù…Øªâ€ŒÚ¯Ø°Ø§Ø±ÛŒ Ú©Ù‡ Ø®Ø¨Ø± Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯
                        
                        # Ø®Ø±ÙˆØ¬ Ø§Ø² Ø­Ù„Ù‚Ù‡ Ú†ÙˆÙ† ÙÙ‚Ø· ÛŒÚ© Ø®Ø¨Ø± Ø¯Ø± Ù‡Ø± Ø¯ÙˆØ± Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒÙ…
                        break
                        
                    except Exception as e:
                        logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø®Ø¨Ø± Ø§Ø² {name}: {e}")
                        err += 1
                        
            except Exception as e:
                logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ù†Ø¨Ø¹ {name}: {e}")
                err += 1

            stats.append({"src": name, "got": got, "sent": sent, "err": err})
            
            # Ø§Ú¯Ø± Ø®Ø¨Ø±ÛŒ Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯ØŒ Ø§Ø² Ø¨Ù‚ÛŒÙ‡ Ù…Ù†Ø§Ø¨Ø¹ ØµØ±Ù Ù†Ø¸Ø± Ú©Ù†
            if news_sent_in_cycle:
                break

    # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø±Ø¯ÛŒØ§Ø¨ÛŒ
    sent_urls |= new_urls
    sent_hashes |= new_h
    save_set(sent_urls, FILES["urls"])
    save_set(sent_hashes, FILES["hashes"])
    save_set(bad, FILES["bad"])

    # ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´
    generate_report(bot, chat_id, stats, news_sent_in_cycle)

async def generate_report(bot, chat_id, stats, news_sent):
    """ØªÙˆÙ„ÛŒØ¯ Ùˆ Ø§Ø±Ø³Ø§Ù„ Ú¯Ø²Ø§Ø±Ø´ Ø¬Ø§Ù…Ø¹"""
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ù„ Ø¢Ù…Ø§Ø±
    total_sources = len(stats)
    total_got = sum(s["got"] for s in stats)
    total_sent = sum(s["sent"] for s in stats)
    total_err = sum(s["err"] for s in stats)
    
    # Ø³Ø§Ø®Øª Ø¬Ø¯ÙˆÙ„ Ú¯Ø²Ø§Ø±Ø´
    hdr = ["Ù…Ù†Ø¨Ø¹", "Ø¯Ø±ÛŒØ§ÙØª", "Ø§Ø±Ø³Ø§Ù„", "Ø®Ø·Ø§"]
    w = {"Ù…Ù†Ø¨Ø¹": 20, "Ø¯Ø±ÛŒØ§ÙØª": 7, "Ø§Ø±Ø³Ø§Ù„": 6, "Ø®Ø·Ø§": 4}
    
    lines = [
        "ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ø¯ÙˆØ± Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø®Ø¨Ø§Ø±",
        f"ğŸ”„ Ú©Ù„ Ù…Ù†Ø§Ø¨Ø¹ Ø¨Ø±Ø±Ø³ÛŒ Ø´Ø¯Ù‡: {total_sources}",
        f"ğŸ“° Ú©Ù„ Ø§Ø®Ø¨Ø§Ø± ÛŒØ§ÙØª Ø´Ø¯Ù‡: {total_got}",
        f"âœ… ØªØ¹Ø¯Ø§Ø¯ Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯Ù‡: {total_sent}",
        f"âŒ ØªØ¹Ø¯Ø§Ø¯ Ø®Ø·Ø§: {total_err}",
        "",
        "  ".join(f"{h:<{w[h]}}" for h in hdr),
        "  ".join("-" * w[h] for h in hdr)
    ]
    
    for r in stats:
        src_name = r["src"]
        if len(src_name) > 18:
            src_name = src_name[:15] + "..."
            
        lines.append("  ".join([
            f"{src_name:<{w['Ù…Ù†Ø¨Ø¹']}}",
            f"{r['got']:>{w['Ø¯Ø±ÛŒØ§ÙØª']}}",
            f"{r['sent']:>{w['Ø§Ø±Ø³Ø§Ù„']}}",
            f"{r['err']:>{w['Ø®Ø·Ø§']}}"
        ]))
    
    lines.append("")
    if news_sent:
        lines.append("âœ… Ø¯Ø± Ø§ÛŒÙ† Ø¯ÙˆØ± ÛŒÚ© Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯")
    else:
        lines.append("â„¹ï¸ Ø¯Ø± Ø§ÛŒÙ† Ø¯ÙˆØ± Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯")
    
    lines.append("â° Ø¯ÙˆØ± Ø¨Ø¹Ø¯ÛŒ ØªØ§ 3 Ø¯Ù‚ÛŒÙ‚Ù‡ Ø¯ÛŒÚ¯Ø±...")
    
    report = "<pre>" + "\n".join(lines) + "</pre>"
    logging.info("ğŸ“‘ Ø§Ø±Ø³Ø§Ù„ Ú¯Ø²Ø§Ø±Ø´ Ø¬Ø§Ù…Ø¹")
    await safe_send(bot, chat_id, report, parse_mode="HTML")
