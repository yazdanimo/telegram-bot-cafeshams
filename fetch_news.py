# File: fetch_news.py â€” Ù†Ø³Ø®Ù‡Ù” Ù†Ù‡Ø§ÛŒÛŒ Ù…Ø±ØªØ¨ Ùˆ Ø¨Ø¯ÙˆÙ† Ø®Ø·Ø§ÛŒ Ù†Ø­ÙˆÛŒ

import aiohttp
import asyncio
import json
import time
import hashlib
import feedparser
import re
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urlunparse, parse_qsl
from utils import load_sources, extract_full_content, summarize_text, format_news

# Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ÙˆØ¶Ø¹ÛŒØª Ùˆ Ú¯Ø²Ø§Ø±Ø´
SEND_INTERVAL      = 3
LAST_SEND          = 0
SENT_URLS_FILE     = "sent_urls.json"
SENT_HASHES_FILE   = "sent_hashes.json"
BAD_LINKS_FILE     = "bad_links.json"
SKIPPED_LOG_FILE   = "skipped_items.json"
GARBAGE_NEWS_FILE  = "garbage_news.json"

def load_set(path):
    """
    ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÛŒÚ© Ù„ÛŒØ³Øª Ø§Ø² ÙØ§ÛŒÙ„ Ùˆ ØªØ¨Ø¯ÛŒÙ„ Ø¢Ù† Ø¨Ù‡ set.
    Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§ØŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡Ù” Ø®Ø§Ù„ÛŒ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
    """
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
            return set(data)
    except Exception:
        return set()

def save_set(data, path):
    """
    Ø°Ø®ÛŒØ±Ù‡Ù” ÛŒÚ© set Ø¨Ù‡ ØµÙˆØ±Øª Ù„ÛŒØ³Øª Ø¯Ø± ÙØ§ÛŒÙ„ JSON.
    """
    with open(path, "w", encoding="utf-8") as f:
        json.dump(list(data), f, ensure_ascii=False, indent=2)

def normalize_url(url):
    """
    Ø­Ø°Ù Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ UTM Ùˆ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ URL.
    """
    p = urlparse(url)
    qs = [(k, v) for k, v in parse_qsl(p.query) if not k.startswith("utm_")]
    query = "&".join(f"{k}={v}" for k, v in qs)
    return urlunparse((p.scheme, p.netloc, p.path, "", query, ""))

def is_garbage(text):
    """
    ÙÛŒÙ„ØªØ± Ù…Ø­ØªÙˆØ§ÛŒ Ø¨ÛŒâ€ŒÚ©ÛŒÙÛŒØª Ø¨Ø± Ø§Ø³Ø§Ø³ Ø·ÙˆÙ„ØŒ
    Ù†Ø³Ø¨Øª Ù†ÙˆÛŒØ³Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒØŒ ØªÚ©Ø±Ø§Ø± Ú©Ø§Ø±Ø§Ú©ØªØ±ØŒ
    Ùˆ Ù†Ø´Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ ØºÛŒØ±Ù…Ø¹Ù…ÙˆÙ„.
    """
    text = text.strip()
    if len(text) < 60:
        return True

    persian = re.findall(r'[\u0600-\u06FF]', text)
    if len(persian) / max(len(text), 1) < 0.4:
        return True

    if re.search(r'(.)\1{5,}', text):
        return True

    # ØªØ¹Ø¯Ø§Ø¯ Ø±Ø´ØªÙ‡â€ŒÙ‡Ø§ÛŒ Ù„Ø§ØªÛŒÙ† Ø·ÙˆÙ„Ø§Ù†ÛŒ
    latin_words = re.findall(r'[A-Za-z]{4,}', text)
    if len(latin_words) > 5 and len(persian) < 50:
        return True

    # Ú©Ù„Ù…Ø§Øª Ù…Ø¹Ù…ÙˆÙ„ ØµÙØ­Ø§Øª ÙØ±Ù… ÛŒØ§ Ø«Ø¨Øªâ€ŒÙ†Ø§Ù…
    for kw in ["Ø«Ø¨Øª Ù†Ø§Ù…", "login", "register", "signup"]:
        if kw in text:
            return True

    return False

def log_garbage(source, link, title, content):
    """
    Ø°Ø®ÛŒØ±Ù‡Ù” Ù…ÙˆØ§Ø±Ø¯ Ù…Ø­ØªÙˆØ§ÛŒ Ø®Ø±Ø§Ø¨ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²Ø¨ÛŒÙ†ÛŒ.
    """
    try:
        with open(GARBAGE_NEWS_FILE, "r", encoding="utf-8") as f:
            items = json.load(f)
    except Exception:
        items = []

    items.append({
        "source": source,
        "link": link,
        "title": title,
        "content": content[:300]
    })
    with open(GARBAGE_NEWS_FILE, "w", encoding="utf-8") as f:
        json.dump(items, f, ensure_ascii=False, indent=2)

def log_skipped(source, url, reason, title=None):
    """
    Ø°Ø®ÛŒØ±Ù‡Ù” Ù…ÙˆØ§Ø±Ø¯ Ø±Ø¯Ø´Ø¯Ù‡ (ØªÚ©Ø±Ø§Ø±ÛŒØŒ Ø®Ø±Ø§Ø¨ØŒ Ø®Ø·Ø§) Ø¨Ø±Ø§ÛŒ Ú¯Ø²Ø§Ø±Ø´.
    """
    try:
        with open(SKIPPED_LOG_FILE, "r", encoding="utf-8") as f:
            items = json.load(f)
    except Exception:
        items = []

    items.append({
        "source": source,
        "url": url,
        "title": title,
        "reason": reason
    })
    with open(SKIPPED_LOG_FILE, "w", encoding="utf-8") as f:
        json.dump(items, f, ensure_ascii=False, indent=2)

async def safe_send(bot, chat_id, text, **kwargs):
    """
    Ø§Ø±Ø³Ø§Ù„ Ù¾ÛŒØ§Ù… Ø¨Ø§ Ø±Ø¹Ø§ÛŒØª ÙØ§ØµÙ„Ù‡Ù” Ø²Ù…Ø§Ù†ÛŒ Ø¨ÛŒÙ† Ø¯Ùˆ Ø§Ø±Ø³Ø§Ù„.
    """
    global LAST_SEND
    elapsed = time.time() - LAST_SEND
    if elapsed < SEND_INTERVAL:
        await asyncio.sleep(SEND_INTERVAL - elapsed)
    try:
        return await bot.send_message(chat_id=chat_id, text=text, **kwargs)
    except Exception as e:
        print("âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±Ø³Ø§Ù„ Ù¾ÛŒØ§Ù…:", e)
    finally:
        LAST_SEND = time.time()

async def parse_rss_async(url):
    """
    Ø®ÙˆØ§Ù†Ø¯Ù† RSS Ø¨Ù‡ ØµÙˆØ±Øª async Ø¨Ø§ timeout.
    """
    try:
        dp = await asyncio.wait_for(
            asyncio.to_thread(feedparser.parse, url),
            timeout=10
        )
        return dp.entries or []
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† RSS {url}:", e)
        return []

async def fetch_html(session, url):
    """
    Ø¯Ø±ÛŒØ§ÙØª HTML ÛŒÚ© ØµÙØ­Ù‡ Ø¨Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ø¯ ÙˆØ¶Ø¹ÛŒØª HTTP.
    """
    async with session.get(url) as res:
        if res.status != 200:
            raise Exception(f"HTTP {res.status}")
        return await res.text()

async def fetch_and_send_news(bot, chat_id, sent_urls, sent_hashes):
    """
    ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ: Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ RSSØŒ
    ÙÛŒÙ„ØªØ± Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ØŒ Ù…Ø¯ÛŒØ±ÛŒØª fallbackØŒ
    Ø§Ø±Ø³Ø§Ù„ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ Ùˆ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ.
    """
    bad_links  = load_set(BAD_LINKS_FILE)
    stats      = []
    sent_now   = set()
    hashes_now = set()

    async with aiohttp.ClientSession(
        timeout=aiohttp.ClientTimeout(total=20)
    ) as session:
        for src in load_sources():
            name   = src.get("name")
            rss    = src.get("rss")
            fb     = src.get("fallback")
            sent   = 0
            err    = 0

            # Û±. Ù¾Ø±Ø¯Ø§Ø²Ø´ RSS
            items = await parse_rss_async(rss)
            total = len(items)
            print(f"ğŸ“¥ Ø¯Ø±ÛŒØ§ÙØª {total} Ø¢ÛŒØªÙ… Ø§Ø² {name}")

            for item in items[:3]:
                link = item.get("link") or ""
                u = normalize_url(link)

                if not u or u in sent_urls or u in sent_now or u in bad_links:
                    log_skipped(name, u, "URL ØªÚ©Ø±Ø§Ø±ÛŒ", item.get("title"))
                    continue

                try:
                    html = await fetch_html(session, link)
                    full = extract_full_content(html)
                    summ = summarize_text(full)

                    if is_garbage(full) or is_garbage(summ):
                        log_skipped(name, u, "Ù…Ø­ØªÙˆØ§ÛŒ Ø¨ÛŒâ€ŒÚ©ÛŒÙÛŒØª", item.get("title"))
                        log_garbage(name, link, item.get("title", ""), full)
                        bad_links.add(u)
                        err += 1
                        continue

                    cap = format_news(name, item.get("title", ""), summ, link)
                    h   = hashlib.md5(cap.encode("utf-8")).hexdigest()

                    if h in sent_hashes or h in hashes_now:
                        log_skipped(name, u, "Ø®Ø±ÙˆØ¬ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ", item.get("title"))
                        continue

                    await safe_send(bot, chat_id, cap, parse_mode="HTML")
                    sent_now.add(u)
                    hashes_now.add(h)
                    sent += 1

                except Exception as e:
                    log_skipped(name, u, f"Ø®Ø·Ø§: {e}", item.get("title"))
                    print("âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´", link, "â†’", e)
                    bad_links.add(u)
                    err += 1

            # Û². Ù…Ø¯ÛŒØ±ÛŒØª fallback Ø§Ú¯Ø± RSS Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯
            if total == 0 and fb:
                try:
                    html_index = await fetch_html(session, fb)
                    soup = BeautifulSoup(html_index, "html.parser")
                    base = urlparse(fb)
                    links = []

                    # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ ØªØ§ Û³ Ù„ÛŒÙ†Ú© ÛŒÚ©ØªØ§ Ø§Ø² Ø¢Ø±Ø´ÛŒÙˆ
                    for a in soup.find_all("a", href=True):
                        href = a["href"]
                        if href.startswith("/"):
                            href = urlunparse(
                                (base.scheme, base.netloc, href, "", "", "")
                            )
                        if urlparse(href).netloc == base.netloc and href not in links:
                            links.append(href)
                        if len(links) >= 3:
                            break

                    for link in links:
                        u = normalize_url(link)
                        if not u or u in sent_urls or u in sent_now or u in bad_links:
                            log_skipped(name, u, "fallback ØªÚ©Ø±Ø§Ø±ÛŒ", "fallback")
                            continue

                        try:
                            html = await fetch_html(session, link)
                            full = extract_full_content(html)
                            summ = summarize_text(full)

                            if is_garbage(full) or is_garbage(summ):
                                log_skipped(name, u, "fallback Ø¨ÛŒâ€ŒÚ©ÛŒÙÛŒØª", "fallback")
                                log_garbage(name, link, "fallback", full)
                                bad_links.add(u)
                                err += 1
                                continue

                            cap = format_news(
                                f"{name} - fallback",
                                "fallback",
                                summ,
                                link
                            )
                            h = hashlib.md5(cap.encode("utf-8")).hexdigest()

                            if h in sent_hashes or h in hashes_now:
                                log_skipped(name, u, "fallback ØªÚ©Ø±Ø§Ø±ÛŒ", "fallback")
                                continue

                            await safe_send(bot, chat_id, cap, parse_mode="HTML")
                            hashes_now.add(h)
                            sent += 1

                        except Exception as fe:
                            log_skipped(name, link, f"Ø®Ø·Ø§ Ø¯Ø± fallback: {fe}", "fallback")
                            print("âŒ Ø®Ø·Ø§ Ø¯Ø± fallback", name, "â†’", fe)
                            bad_links.add(u)
                            err += 1

                except Exception as e:
                    log_skipped(name, fb, f"fallback index error: {e}")
                    print("âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª fallback index:", e)
                    bad_links.add(fb)
                    err += 1

            stats.append({
                "Ù…Ù†Ø¨Ø¹": name,
                "Ø¯Ø±ÛŒØ§ÙØª": total,
                "Ø§Ø±Ø³Ø§Ù„": sent,
                "Ø®Ø·Ø§": err
            })

        # Ø°Ø®ÛŒØ±Ù‡Ù” Ù†Ù‡Ø§ÛŒÛŒ ÙˆØ¶Ø¹ÛŒØª
        sent_urls.update(sent_now)
        sent_hashes.update(hashes_now)
        save_set(sent_urls, SENT_URLS_FILE)
        save_set(sent_hashes, SENT_HASHES_FILE)
        save_set(bad_links, BAD_LINKS_FILE)

        # Ø³Ø§Ø®Øª Ùˆ Ø§Ø±Ø³Ø§Ù„ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ
        headers = ["Source", "Fetched", "Sent", "Errors"]
        widths  = {h: len(h) for h in headers}
        max_src = max((len(r["Ù…Ù†Ø¨Ø¹"]) for r in stats), default=0)
        widths["Source"] = max(widths["Source"], max_src)

        for r in stats:
            widths["Fetched"] = max(widths["Fetched"], len(str(r["Ø¯Ø±ÛŒØ§ÙØª"])))
            widths["Sent"]    = max(widths["Sent"],    len(str(r["Ø§Ø±Ø³Ø§Ù„"])))
            widths["Errors"]  = max(widths["Errors"],  len(str(r["Ø®Ø·Ø§"])))

        lines = [
            "ğŸ“Š News Aggregation Report:\n",
            "  ".join(f"{h:<{widths[h]}}" for h in headers),
            "  ".join("-" * widths[h] for h in headers)
        ]
        for r in stats:
            row = [
                f"{r['Ù…Ù†Ø¨Ø¹']:<{widths['Source']}}",
                f"{r['Ø¯Ø±ÛŒØ§ÙØª']:>{widths['Fetched']}}",
                f"{r['Ø§Ø±Ø³Ø§Ù„']:>{widths['Sent']}}",
                f"{r['Ø®Ø·Ø§']:>{widths['Errors']}}"
            ]
            lines.append("  ".join(row))

        report = "<pre>" + "\n".join(lines) + "</pre>"
        await safe_send(bot, chat_id, report, parse_mode="HTML")
