import requests
from bs4 import BeautifulSoup
from langdetect import detect
from translatepy import Translator
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer
from utils import extract_full_content, extract_image_from_html
import json
import nltk
import asyncio

# ğŸ§  Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ
nltk.download("punkt")

translator = Translator()

# ğŸ“š Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ù†Ø§Ø¨Ø¹
with open("sources.json", "r", encoding="utf-8") as f:
    sources = json.load(f)

def summarize_text(text, sentence_count=4):
    try:
        parser = PlaintextParser.from_string(text, Tokenizer("english"))
        summary = LsaSummarizer()(parser.document, sentence_count)
        summarized = " ".join(str(sentence) for sentence in summary).strip()
        return summarized if len(summarized) > 100 else text[:400]
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ: {e}")
        return text[:400]

async def fetch_and_send_news(bot, chat_id, sent_urls):
    headers = { "User-Agent": "Mozilla/5.0" }  # Ø¨Ø±Ø§ÛŒ Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø­Ø¯ÙˆØ¯ Ù…Ø«Ù„ Reuters

    for source in sources:
        name = source.get("name")
        url = source.get("url")

        try:
            rss = requests.get(url, timeout=10, headers=headers)
            rss.raise_for_status()
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª RSS Ø§Ø² {name}: {e}")
            continue

        soup = BeautifulSoup(rss.content, "xml")
        items = soup.find_all("item")
        print(f"\nğŸ“¡ Ø¯Ø±ÛŒØ§ÙØª RSS Ø§Ø² {name} â†’ Ù…Ø¬Ù…ÙˆØ¹: {len(items)}")

        for item in items[:3]:
            link = item.link.text.strip() if item.link else ""
            if not link or link in sent_urls:
                continue

            title = item.title.text.strip() if item.title else "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†"
            raw_html = item.description.text.strip() if item.description else ""

            image_url = extract_image_from_html(raw_html)
            full_text, _ = extract_full_content(link)

            # Ø±Ø¯ Ù…Ø­ØªÙˆØ§ÛŒ Ø¶Ø¹ÛŒÙ ÛŒØ§ Ù‚Ø§Ù„Ø¨ Ø³Ø§ÛŒØª
            if not full_text or len(full_text.strip()) < 300:
                print(f"âš ï¸ Ø±Ø¯ Ø´Ø¯: Ù…Ø­ØªÙˆØ§ÛŒ Ù†Ø§Ù‚Øµ Ø§Ø² {name}")
                continue
            garbage_keywords = ["ÙØ§Ø±Ø³ÛŒ", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©", "English", "ØªÙ…Ø§Ø³ Ø¨Ø§ Ù…Ø§", "ØªØ¨Ù„ÛŒØºØ§Øª", "Ø¢Ø±Ø´ÛŒÙˆ", "ÙÛŒØ¯ Ø®Ø¨Ø±", "Ú©Ø¯ Ø§Ø³ØªØ§ØªÙˆØ³", "ØµÙØ­Ù‡ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª"]
            if any(word in full_text for word in garbage_keywords):
                print(f"âš ï¸ Ø±Ø¯ Ø´Ø¯: Ù…Ø­ØªÙˆØ§ÛŒ Ù‚Ø§Ù„Ø¨ ÛŒØ§ Ù…Ù†ÙˆÛŒ Ø³Ø§ÛŒØª Ø§Ø² {name}")
                continue

            try:
                lang = detect(title + full_text)
                if lang == "en":
                    title = translator.translate(title, "Persian").result
                    full_text = translator.translate(full_text, "Persian").result
            except Exception as e:
                print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± ØªØ±Ø¬Ù…Ù‡ ÛŒØ§ Ø²Ø¨Ø§Ù† Ø®Ø¨Ø± Ø§Ø² {name}: {e}")
                continue

            summary = summarize_text(full_text, 4)

            caption = (
                f"ğŸ“¡ Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ {name}\n"
                f"{title}\n\n"
                f"{summary.strip()}\n\n"
                f"ğŸ†” @cafeshamss\nÚ©Ø§ÙÙ‡ Ø´Ù…Ø³ â˜•ï¸ğŸª"
            )

            try:
                if image_url:
                    await bot.send_photo(chat_id=chat_id, photo=image_url, caption=caption[:1024])
                else:
                    await bot.send_message(chat_id=chat_id, text=caption[:4096])
                print(f"âœ… Ø®Ø¨Ø± Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯ Ø§Ø² {name}")
                sent_urls.add(link)
                await asyncio.sleep(2)  # Ú©Ù†ØªØ±Ù„ flood
            except Exception as e:
                print(f"â—ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±Ø³Ø§Ù„ Ø§Ø² {name}: {e}")

    return sent_urls
