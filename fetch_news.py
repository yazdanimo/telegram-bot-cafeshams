import requests
from bs4 import BeautifulSoup
from langdetect import detect
from translatepy import Translator
from utils import extract_full_content, extract_image_from_html
import json
import asyncio

translator = Translator()

# ğŸ‘‡ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ù†Ø§Ø¨Ø¹ Ø¨Ø§ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…ÙˆØ¶ÙˆØ¹ÛŒ
with open("sources.json", "r", encoding="utf-8") as f:
    sources = json.load(f)

# ğŸ“Œ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ† Ø®Ø¨Ø±
def summarize_text(text, max_chars=400):
    paragraphs = [p.strip() for p in text.split("\n") if len(p.strip()) > 50]
    return "\n".join(paragraphs[:3])[:max_chars]

# ğŸ§  Ø§ØµÙ„Ø§Ø­ Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ø®Ø§Øµ Ø¨Ø±Ø§ÛŒ ØªØ±Ø¬Ù…Ù‡ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±
def fix_named_entities(text):
    corrections = {
        "Araqchi": "Ø¹Ø±Ø§Ù‚Ú†ÛŒ",
        "KSA": "Ø¹Ø±Ø¨Ø³ØªØ§Ù† Ø³Ø¹ÙˆØ¯ÛŒ",
        "Aliza Enati": "Ø¹Ù„ÛŒØ²Ø§ Ø§Ù†Ø§ØªÛŒ",
        "Faisal bin Farhan": "ÙÛŒØµÙ„ Ø¨Ù† ÙØ±Ø­Ø§Ù†",
        "Walid bin Abdulkarim Al-Khulaifi": "ÙˆÙ„ÛŒØ¯ Ø¨Ù† Ø¹Ø¨Ø¯Ø§Ù„Ú©Ø±ÛŒÙ… Ø§Ù„Ø®Ù„ÛŒÙÛŒ",
        "Arash Rezavand": "Ø¢Ø±Ø´ Ø±Ø¶Ø§ÙˆÙ†Ø¯",
        "Sepahan": "Ø³Ù¾Ø§Ù‡Ø§Ù†",
        "Patrice Carteron": "Ù¾Ø§ØªØ±ÛŒØ³ Ú©Ø§Ø±ØªØ±ÙˆÙ†",
        "Moharram Navidkia": "Ù…Ø­Ø±Ù… Ù†ÙˆÛŒØ¯Ú©ÛŒØ§",
        "Umm Salal": "Ø§Ù…â€ŒØµÙ„Ø§Ù„"
    }
    for eng, fa in corrections.items():
        text = text.replace(eng, fa)
    return text

# ğŸ§¹ Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ø¹Ø¨Ø§Ø±Ø§Øª ØªÚ©Ø±Ø§Ø±ÛŒ ÛŒØ§ Ø¨ÛŒâ€ŒÙ…Ø¹Ù†Ø§
def clean_messy_phrases(text):
    replacements = [
        "Ø¯Ø± Û±Û² Ø§ÙˆØª Ø¯Ø± Û±Û² Ø§ÙˆØª",
        "Ø¯Ø± ØªØ§Ø±ÛŒØ® 12 Ø§ÙˆØª Ø¯Ø± 12 Ø§ÙˆØª",
        "Ø¨Ø§ Ù¾Ø±Ø¯Ø§Ø®Øª Ù‡Ø²ÛŒÙ†Ù‡ Ù†Ø§Ø¹Ø§Ø¯Ù„Ø§Ù†Ù‡"
    ]
    for phrase in replacements:
        text = text.replace(phrase, "")
    return text

# âœ‚ï¸ Ø­Ø°Ù Ø¬Ù…Ù„Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ù‚Øµ ÛŒØ§ Ú©ÙˆØªØ§Ù‡
def clean_incomplete_sentences(text):
    lines = text.split("\n")
    cleaned = []
    for line in lines:
        if len(line.strip()) < 20 or line.strip().endswith(("...", "Ø¨ÛŒÙ† Ø¯Ùˆ", "ØŒ")):
            continue
        cleaned.append(line.strip())
    return "\n".join(cleaned)

# ğŸŒ ØªØ±Ø¬Ù…Ù‡ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø§ Ø§ØµÙ„Ø§Ø­Ø§Øª
def translate_text(text):
    try:
        raw = fix_named_entities(text)
        messy = clean_messy_phrases(raw)
        cleaned = clean_incomplete_sentences(messy)
        translated = translator.translate(cleaned, "Persian").result
        return translated
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± ØªØ±Ø¬Ù…Ù‡: {e}")
        return text[:400]

# ğŸ“¡ ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ùˆ Ø§Ø±Ø³Ø§Ù„ Ø®Ø¨Ø±
async def fetch_and_send_news(bot, chat_id, sent_urls, category_filter=None):
    headers = { "User-Agent": "Mozilla/5.0" }

    for source in sources:
        name = source.get("name")
        url = source.get("url")
        category = source.get("category", "news")

        if category_filter and category != category_filter:
            continue

        try:
            response = requests.get(url, timeout=10, headers=headers)
            response.raise_for_status()
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª RSS Ø§Ø² {name}: {e}")
            continue

        soup = BeautifulSoup(response.content, "xml")
        items = soup.find_all("item")
        print(f"\nğŸ“¡ Ø¯Ø±ÛŒØ§ÙØª RSS Ø§Ø² {name} â†’ Ù…Ø¬Ù…ÙˆØ¹: {len(items)}")

        for item in items[:3]:
            link = item.link.text.strip() if item.link else ""
            if not link or link in sent_urls:
                continue

            title = item.title.text.strip() if item.title else "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†"
            raw_html = item.description.text.strip() if item.description else ""
            image_url = extract_image_from_html(raw_html)
            full_text, _ = extract_full_content(link)

            if not full_text or len(full_text) < 300:
                print(f"âš ï¸ Ø±Ø¯ Ø´Ø¯: Ù…ØªÙ† Ù†Ø§Ú©Ø§ÙÛŒ ÛŒØ§ Ø¶Ø¹ÛŒÙ Ø§Ø² {name}")
                continue

            garbage_keywords = ["ØªÙ…Ø§Ø³ Ø¨Ø§ Ù…Ø§", "ÙÛŒØ¯ Ø®Ø¨Ø±", "Privacy", "Ø¢Ø±Ø´ÛŒÙˆ", "404", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"]
            if any(word in full_text for word in garbage_keywords):
                print(f"âš ï¸ Ø±Ø¯ Ø´Ø¯: Ù…Ø­ØªÙˆØ§ÛŒ Ù‚Ø§Ù„Ø¨ ÛŒØ§ Ù…Ù†Ùˆ Ø§Ø² {name}")
                continue

            try:
                lang = detect(title + full_text)
                if lang == "en":
                    title = translate_text(title)
                    full_text = translate_text(full_text)
            except Exception as e:
                print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± ØªØ´Ø®ÛŒØµ Ø²Ø¨Ø§Ù† ÛŒØ§ ØªØ±Ø¬Ù…Ù‡ Ø§Ø² {name}: {e}")
                continue

            summary = summarize_text(full_text)

            caption = (
                f"ğŸ“¡ Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ {name} ({category})\n"
                f"{title}\n\n"
                f"{summary.strip()}\n\n"
                f"ğŸ†” @cafeshamss\nÚ©Ø§ÙÙ‡ Ø´Ù…Ø³ â˜•ï¸ğŸª"
            )

            try:
                if image_url:
                    await bot.send_photo(chat_id=chat_id, photo=image_url, caption=caption[:1024])
                else:
                    await bot.send_message(chat_id=chat_id, text=caption[:4096])
                print(f"âœ… Ø®Ø¨Ø± Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯ Ø§Ø² {name}")
                sent_urls.add(link)
                await asyncio.sleep(2)
            except Exception as e:
                print(f"â—ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±Ø³Ø§Ù„ Ø®Ø¨Ø± Ø§Ø² {name}: {e}")

    return sent_urls
