import requests
from bs4 import BeautifulSoup
from langdetect import detect
from translatepy import Translator
from utils import extract_full_content, extract_image_from_html
import json
import asyncio

translator = Translator()

with open("sources.json", "r", encoding="utf-8") as f:
    sources = json.load(f)

# Ø§ØµÙ„Ø§Ø­ Ø§Ø³Ø§Ù…ÛŒ Ø®Ø§Øµ Ùˆ ØºÙ„Ø· ØªØ±Ø¬Ù…Ù‡â€ŒÙ‡Ø§
def fix_named_entities(text):
    corrections = {
        "Araqchi": "Ø¹Ø±Ø§Ù‚Ú†ÛŒ",
        "KSA": "Ø¹Ø±Ø¨Ø³ØªØ§Ù† Ø³Ø¹ÙˆØ¯ÛŒ",
        "Aliza Enati": "Ø¹Ù„ÛŒØ²Ø§ Ø§Ù†Ø§ØªÛŒ",
        "Faisal bin Farhan": "ÙÛŒØµÙ„ Ø¨Ù† ÙØ±Ø­Ø§Ù†",
        "Walid bin Abdulkarim Al-Khulaifi": "ÙˆÙ„ÛŒØ¯ Ø¨Ù† Ø¹Ø¨Ø¯Ø§Ù„Ú©Ø±ÛŒÙ… Ø§Ù„Ø®Ù„ÛŒÙÛŒ",
        "Arash Rezavand": "Ø¢Ø±Ø´ Ø±Ø¶Ø§ÙˆÙ†Ø¯",
        "Sepahan": "Ø³Ù¾Ø§Ù‡Ø§Ù†",
        "Patrice Carteron": "Ù¾Ø§ØªØ±ÛŒØ³ Ú©Ø§Ø±ØªØ±ÙˆÙ†",
        "Moharram Navidkia": "Ù…Ø­Ø±Ù… Ù†ÙˆÛŒØ¯Ú©ÛŒØ§",
        "Umm Salal": "Ø§Ù…â€ŒØµÙ„Ø§Ù„"
    }
    for eng, fa in corrections.items():
        text = text.replace(eng, fa)
    return text

# Ø­Ø°Ù Ø¹Ø¨Ø§Ø±Øªâ€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ ÛŒØ§ Ù†Ø§Ù‚Øµ
def clean_messy_phrases(text):
    replacements = [
        "Ø¯Ø± Û±Û² Ø§ÙˆØª Ø¯Ø± Û±Û² Ø§ÙˆØª",
        "Ø¯Ø± ØªØ§Ø±ÛŒØ® 12 Ø§ÙˆØª Ø¯Ø± 12 Ø§ÙˆØª",
        "Ø¨Ø§ Ù¾Ø±Ø¯Ø§Ø®Øª Ù‡Ø²ÛŒÙ†Ù‡ Ù†Ø§Ø¹Ø§Ø¯Ù„Ø§Ù†Ù‡"
    ]
    for phrase in replacements:
        text = text.replace(phrase, "")
    return text

# ØªØ´Ø®ÛŒØµ Ø¬Ù…Ù„Ù‡ Ù†Ø§Ù‚Øµ
def is_incomplete(text):
    bad_endings = ["...", "ØŒ", "Ø¨ÛŒÙ† Ø¯Ùˆ", "Ø¨Ø±Ø§ÛŒ Ú¯Ø³ØªØ±Ø´", "Ø¯Ø± Ø­Ø§Ù„ÛŒ Ú©Ù‡", "Ø²ÛŒØ±Ø§", "ØªØ§", "Ùˆ", "Ú©Ù‡"]
    return any(text.strip().endswith(ending) for ending in bad_endings)

# Ø­Ø°Ù Ø¬Ù…Ù„Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ù‚Øµ Ø§Ø² Ù…ØªÙ†
def clean_incomplete_sentences(text):
    lines = text.split("\n")
    cleaned = []
    for line in lines:
        if len(line.strip()) < 30 or is_incomplete(line):
            continue
        cleaned.append(line.strip())
    return "\n".join(cleaned)

# Ø­Ø°Ù Ø¬Ù…Ù„Ù‡Ù” Ø¢Ø®Ø± Ø§Ú¯Ø± Ù†Ø§Ù‚Øµ Ø¨ÙˆØ¯
def fix_cutoff_translation(text):
    lines = text.split("\n")
    if lines and is_incomplete(lines[-1]):
        print("âš ï¸ Ø¬Ù…Ù„Ù‡Ù” Ø¢Ø®Ø± Ù†Ø§Ù‚Øµ Ø¨ÙˆØ¯ØŒ Ø­Ø°Ù Ø´Ø¯.")
        return "\n".join(lines[:-1])
    return text

# ØªØ±Ø¬Ù…Ù‡ Ú©Ø§Ù…Ù„ Ù…ØªÙ†
def translate_text(text):
    try:
        raw = fix_named_entities(text)
        messy = clean_messy_phrases(raw)
        cleaned = clean_incomplete_sentences(messy)
        translated = translator.translate(cleaned, "Persian").result
        return fix_cutoff_translation(translated)
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± ØªØ±Ø¬Ù…Ù‡: {e}")
        return text[:400]

# Ø¨Ø±Ø±Ø³ÛŒ Ú©ÛŒÙÛŒØª Ø§ÙˆÙ„ÛŒÙ‡ Ù…ØªÙ†
def assess_content_quality(text):
    paragraph_count = len([p for p in text.split("\n") if len(p.strip()) > 40])
    character_count = len(text)
    return character_count >= 300 and paragraph_count >= 2

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙÙ‚Ø· Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù Ø§ÙˆÙ„ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´
def extract_intro_paragraph(text):
    for para in text.split("\n"):
        if len(para.strip()) > 50:
            return para.strip()
    return text.strip()[:300]

# Ø¯Ø±ÛŒØ§ÙØª Ùˆ Ø§Ø±Ø³Ø§Ù„ Ø®Ø¨Ø±Ù‡Ø§
async def fetch_and_send_news(bot, chat_id, sent_urls, category_filter=None):
    headers = {"User-Agent": "Mozilla/5.0"}

    for source in sources:
        name = source.get("name")
        url = source.get("url")
        category = source.get("category", "news")
        content_type = source.get("content_type", "text")

        if category_filter and category != category_filter:
            continue

        try:
            response = requests.get(url, timeout=10, headers=headers)
            response.raise_for_status()
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª RSS Ø§Ø² {name}: {e}")
            continue

        soup = BeautifulSoup(response.content, "xml")
        items = soup.find_all("item")
        print(f"\nğŸ“¡ Ø¯Ø±ÛŒØ§ÙØª RSS Ø§Ø² {name} â†’ Ù…Ø¬Ù…ÙˆØ¹: {len(items)}")

        for item in items[:5]:
            link = item.link.text.strip() if item.link else ""
            if not link or link in sent_urls:
                continue

            title = item.title.text.strip() if item.title else "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†"
            raw_html = item.description.text.strip() if item.description else ""
            image_url = extract_image_from_html(raw_html)
            full_text, _ = extract_full_content(link)

            if not assess_content_quality(full_text):
                print(f"âš ï¸ Ø±Ø¯ Ø´Ø¯: Ú©ÛŒÙÛŒØª Ù…ØªÙ† Ù¾Ø§ÛŒÛŒÙ† Ø§Ø² {name}")
                continue

            garbage_keywords = ["ØªÙ…Ø§Ø³ Ø¨Ø§ Ù…Ø§", "ÙÛŒØ¯ Ø®Ø¨Ø±", "Privacy", "Ø¢Ø±Ø´ÛŒÙˆ", "404", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"]
            if any(word in full_text for word in garbage_keywords):
                print(f"âš ï¸ Ø±Ø¯ Ø´Ø¯: Ù…Ø­ØªÙˆØ§ÛŒ Ù‚Ø§Ù„Ø¨ ÛŒØ§ ØªØ¨Ù„ÛŒØº Ø§Ø² {name}")
                continue

            try:
                lang = detect(title + full_text)
                if lang == "en":
                    title = translate_text(title)
                    full_text = translate_text(full_text)
            except Exception as e:
                print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± ØªØ´Ø®ÛŒØµ Ø²Ø¨Ø§Ù† ÛŒØ§ ØªØ±Ø¬Ù…Ù‡ Ø§Ø² {name}: {e}")
                continue

            clean_text = clean_incomplete_sentences(full_text)
            intro = extract_intro_paragraph(clean_text)

            caption = (
                f"ğŸ“¡ Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ {name} ({category})\n"
                f"{title}\n\n"
                f"{intro}\n\n"
                f"ğŸ†” @cafeshamss\nÚ©Ø§ÙÙ‡ Ø´Ù…Ø³ â˜•ï¸ğŸª"
            )

            try:
                if image_url:
                    await bot.send_photo(chat_id=chat_id, photo=image_url, caption=caption[:1024])
                else:
                    await bot.send_message(chat_id=chat_id, text=caption[:4096])
                print(f"âœ… Ø®Ø¨Ø± Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯ Ø§Ø² {name}")
                sent_urls.add(link)
                await asyncio.sleep(2)
            except Exception as e:
                print(f"â—ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±Ø³Ø§Ù„ Ø®Ø¨Ø± Ø§Ø² {name}: {e}")

    print(f"\nğŸ“Š Ù…Ø¬Ù…ÙˆØ¹ Ø§Ø±Ø³Ø§Ù„â€ŒØ´Ø¯Ù‡â€ŒÙ‡Ø§: {len(sent_urls)}")
    return sent_urls
